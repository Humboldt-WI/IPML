{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9e8d57",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/IPML/blob/master/tutorial_notebooks/4_rpf_case_study.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679e745-23cc-4834-bd5f-b09122c84231",
   "metadata": {},
   "source": [
    "# Resale Price Forecasting Case Study\n",
    "The lecture introduced you to resale price forecasting, a task to support decision-making in the leasing business. In this tutorial, we will explore a dataset representing the resale price use case prediction. In this scope, the tutorial introduces the Pandas library for data manipulation and analysis. We will cover programming practices related to this library, including data loading and selection. Furthermore, we emphasize data visualization and exploratory data analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ec83c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13177d55-c2b2-4c9e-90b2-54b95480ac10",
   "metadata": {},
   "source": [
    "# Loading  the Resale Price Prediction Dataset\n",
    "You can find the Resale Price Prediction dataset for this notebook on our Moodle page and in our GitHub repository. The Resale Price Prediction dataset focuses on laptops that have been leased and returned, aiming to predict their resale prices. The resale price is influenced by various factors, including the original retail price, depreciation, release year, screen size, hard drive size, RAM size, weight, lease duration, and battery capacity. Our final goal is to use the features to forecast resale prices. For start, however, we will explore the data to better understand its characteristics. Furthermore, data exploration facilitates learning about relevant Python libraries. For example, we will use the `pandas` library. Pandas is a widely used library for data analysis and manipulation in Python, providing powerful tools for handling structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b811f1b-651f-47eb-8c68-41c563f6eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # load Pandas library, the go-to library for working with data sets in Python\n",
    "import os\n",
    "\n",
    "# We put the data on the web so you can download it easily\n",
    "url = 'https://raw.githubusercontent.com/Humboldt-WI/IPML/main/data/resale_price_dataset.csv'\n",
    "\n",
    "# We will use pandas read_csv method to read data right from the web\n",
    "data = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2035a84",
   "metadata": {},
   "source": [
    "Note that loading data from the web is not always reliable, as the data source may change or become unavailable. Therefore, we recommend downloading the dataset from our GitHub repository and saving it in a local `data` folder within your project directory. This way, you can ensure that you have a stable copy of the dataset for your analysis.\n",
    "```python\n",
    "import os  # library for handling file paths\n",
    "# Define where on your hard drive the data file is located. You may need to adjust the path below.\n",
    "url = os.path.join(os.getcwd(), 'data', 'resale_price_dataset.csv')\n",
    "print(f\"Try to load data from: {url}\")   \n",
    "# Loading the data works the same way as before. If you encounter errors, please check the file path above.\n",
    "data = pd.read_csv(url) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa51e8c-27b6-49ef-8d32-e76c7ac8d3b5",
   "metadata": {},
   "source": [
    "Let's first take a look at the data. To that end, we use the function `.head()`, which creates a preview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67ac57-9e57-4c7f-a179-4d2cb6164f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The head function is used to display the first few rows of the dataset after it has been loaded into a pandas DataFrame format\n",
    "data.head(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e8bbb-a3e2-4fc2-ab57-80ffdd796b31",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "The pandas library offers various functions to compute descriptive statistics, which help us summarize and understand the main characteristics of our dataset. Descriptive statistics provide insights into the distribution, central tendency, and variability of the data, allowing us to quickly grasp its overall structure. Furthermore, the pandas library offers functions to understand the data types and identify missing values in our dataset.\n",
    "\n",
    "The relevant functions we will use for our first examination of the data are `pd.DataFrame.info()` and `pd.DataFrame.describe()`.\n",
    "\n",
    "The `pd.DataFrame.info()` function reveals the high-level structure of our data table. Note that pandas uses the term *data frame* to refer to a table. The `pd.DataFrame.info()` function provides information on the number of entries (eg, rows), the data types of each column, and the number of missing values if any. Understanding these details is crucial for further analysis of the data. \n",
    "\n",
    "The `pd.DataFrame.describe()` function computes a suite of summary statistics for each column in our dataset. Examples include the average of a column or its standard deviation. \n",
    "\n",
    "But why are these functions essential? Both ```info()``` and ```describe()``` help us establish a foundational understanding of our dataset's distribution, scale, and tendencies. While ```info()``` gives us a structural overview, ```describe()``` takes us a step further into the statistical nature of each column. By noting aspects like the mean, standard deviation, and minimum/maxium values, we can swiftly detect outliers, identify patterns, and formulate hypotheses for further investigation.\n",
    "\n",
    "Together, these methods serve as our initial checkpoint, ensuring that we're not only aware of the dataset's composition but also acquainted with its statistical properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072e1e0-6281-4319-84d9-811155906fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain information about the structure and\n",
    "# characteristics of the dataset using the \n",
    "# .info() method\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0107097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count, Mean, std, min max,  \n",
    "# and quartiles for numeric columns\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d54f9a",
   "metadata": {},
   "source": [
    "# Data selection\n",
    "Pandas dataframes work similar to Python lists. You can select data items using indexing. Given that a dataframe is a two-dimensional data structure, we need to distinguish between selecting rows or columns. \n",
    "\n",
    "The easist way to index a dataframe is to use the column name. For example, to select the column `resale_price`, we can use the following code:\n",
    "```python   \n",
    "resale_prices = data['Observed resale_price']\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca21f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column 'Observed resale_price' from the dataframe\n",
    "resale_prices = data['Observed resale price']\n",
    "resale_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns by passing a list of column names\n",
    "cols = ['Model', 'RAM size (GB)', 'Retail price']\n",
    "subset = data[cols]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342dbe0",
   "metadata": {},
   "source": [
    "## Advanced indexing using .loc and .iloc\n",
    "The functions `.loc` and `.iloc` provide more advanced indexing capabilities. The `.loc` function allows you to select data based on labels, while `.iloc` allows you to select data based on integer positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index first 3 rows, and a subset of columns\n",
    "subset = data.loc[0:3, ['Model', 'RAM size (GB)']]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56461377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns of row 13\n",
    "subset = data.loc[13, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28470130",
   "metadata": {},
   "source": [
    "The above examples using `.loc[]` would not facilitate selecting columns based on their integer positions. For that, we would use `.iloc[]`. For example, to select all rows of the 1st, 3rd, and 7th columns, we could write:\n",
    "```python   \n",
    "subset = data.iloc[:, [1, 3, 7]]    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7295960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index all rows,\n",
    "# and column 1, 3, and 7\n",
    "subset = data.iloc[:, [1, 3, 7]]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865ee7a",
   "metadata": {},
   "source": [
    "The function `.loc` and `.iloc` may look similar. Note an import difference. The `.loc` function uses the actual labels of the rows and columns, while `.iloc` uses their integer positions. For columns, it is normal to distinguish between the two as column names (typically) consist of text. Rows, on the other hand, may not have more than a consecutive number as row index. This is also what you see in the above example. In this case - and only in this case - the row index and the integer position are identical.\n",
    "```python\n",
    "# Index first 5 rows and all columns\n",
    "subset1 = data.iloc[0:5, :]\n",
    "# Do the same using .loc\n",
    "subset2 = data.loc[0:4, :]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index first 5 rows and the second column\n",
    "subset1 = data.iloc[0:5, 1]\n",
    "subset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index first 5 rows and the second column\n",
    "subset2 = data.loc[0:4, 'Model']\n",
    "subset2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f465a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Drawing of the previous examples on how to index dataframes using `.loc` and `.iloc`, you can work on the following exercises to advance your understanding of the Pandas library and our dataset. To avoid accidental modification of the original data, we first create a copy of the data frame and and recommend that you use the variable `df` for all exercise tasks. If, at some points, you want to restore the original data, simply execute:\n",
    "```python\n",
    "df = data.copy()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()  # create a copy of the original data frame and store it in the variable df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b5b37",
   "metadata": {},
   "source": [
    "### Exercise 1 – Getting to Know the Data Frame\n",
    "\n",
    "1. Display the first 5 rows of `df`.\n",
    "2. Check how many rows and columns the data frame has.\n",
    "3. Print the list of column names.\n",
    "4. Select one column of your choice as a **Series** and then the same column as a **DataFrame**.\n",
    "\n",
    "*Hints:*\n",
    "- Use `df.head()`, `df.shape`, and `df.columns`.\n",
    "- To select a column as a Series: `df[\"some_column\"]`.\n",
    "- To select a column as a DataFrame: `df[[\"some_column\"]]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7593c3",
   "metadata": {},
   "source": [
    "### Exercise 2 – Row Selection with `.iloc`\n",
    "\n",
    "1. Select the first 10 rows of `df` using `.iloc`.\n",
    "2. Select rows 10 to 19 (i.e., the 11th to 20th rows) using `.iloc`.\n",
    "3. Select the last row of `df` using `.iloc`.\n",
    "\n",
    "*Hints:*\n",
    "- `.iloc` is **purely position-based indexing**: `.iloc[rows, columns]`.\n",
    "- You can slice rows with `df.iloc[start:stop]`.\n",
    "- Negative indices work as in Python lists, e.g. `df.iloc[-1]` for the last row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c76568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408608e",
   "metadata": {},
   "source": [
    "### Exercise 3 – Column and Row Subsets with `.iloc`\n",
    "\n",
    "1. Using `.iloc`, select:\n",
    "   - The first 5 rows and the first 3 columns.\n",
    "   - Every second row from the first 20 rows (i.e., row positions 0, 2, 4, …, 18) and the first 2 columns.\n",
    "2. Store one of these subsets in a variable called `df_small` and display it.\n",
    "\n",
    "*Hints:*\n",
    "- General pattern: `df.iloc[row_slice, column_slice]`.\n",
    "- You can use step sizes in slices: `df.iloc[0:20:2]` selects every 2nd row from position 0 to 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79612098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3601c0f",
   "metadata": {},
   "source": [
    "### Exercise 4 – Label-based Indexing with `.loc`\n",
    "\n",
    "1. Inspect the **index** of `df` using `df.index`.\n",
    "2. Store the first three index labels in a variable, e.g.  \n",
    "   `first_labels = df.index[:3]`.\n",
    "3. Use `.loc` to select the rows with these three index labels.\n",
    "4. Use `.loc` to select **two columns of your choice** for these same rows.\n",
    "\n",
    "*Hints:*\n",
    "- `.loc` uses **labels**, not positions: `.loc[row_labels, column_labels]`.\n",
    "- You can pass lists of labels: `df.loc[[\"label1\", \"label2\"], [\"colA\", \"colB\"]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5221809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4df4",
   "metadata": {},
   "source": [
    "### Exercise 5 – Mixing `.loc` and `.iloc`\n",
    "\n",
    "1. Using `.iloc`, select any **single row** and store it in a variable, e.g. `row_pos = 10`.\n",
    "2. Use `.iloc` to view that row.\n",
    "3. Now take the **index label** of that row (e.g. `row_label = df.index[row_pos]`) and use `.loc` to retrieve the same row.\n",
    "4. Compare the results from `.iloc` and `.loc`. Are they identical?\n",
    "\n",
    "*Hints:*\n",
    "- `.iloc[row_pos]` → uses integer position.\n",
    "- `.loc[row_label]` → uses the label from the index.\n",
    "- This exercise is about understanding that the **position** and the **label** are conceptually different (even if they sometimes coincide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cc073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5baf8",
   "metadata": {},
   "source": [
    "### Exercise 6 – Understanding `.loc` vs `.iloc` Through Summary Statistics\n",
    "\n",
    "This exercise deepens your understanding of the difference between **position-based** indexing (`.iloc`) and **label-based** indexing (`.loc`) by working with summary statistics.\n",
    "\n",
    "1. Use `df.describe()` to compute summary statistics for all numeric columns.  \n",
    "   Store the result in a variable called `summary`.\n",
    "\n",
    "2. Display `summary` to inspect the index labels it uses (e.g., `\"count\"`, `\"mean\"`, `\"25%\"`, etc.).\n",
    "\n",
    "3. Extract the **first quartile row** (the `\"25%\"` row) from `summary` **using `.loc`**.\n",
    "\n",
    "4. Try to extract the same row using `.iloc`.  \n",
    "   - Before doing so, inspect the index of `summary` (via `summary.index`) to find the correct **position** of the `\"25%\"` row.\n",
    "\n",
    "5. Compare both results. Are they identical?  \n",
    "   Briefly explain in a markdown cell why the following statement is true:  \n",
    "   > *“.loc retrieves values using **labels**, while .iloc retrieves values using **integer positions** — even if the labels look like numbers.”*\n",
    "\n",
    "*Hints:*\n",
    "- Step 3 works like: `summary.loc[\"25%\"]`.\n",
    "- Step 4 requires something like: `summary.iloc[position]`, where `position` is the integer index corresponding to `\"25%\"`.\n",
    "- Use `summary.index` to inspect the index labels and determine their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e3eb0",
   "metadata": {},
   "source": [
    "### Exercise 7 – Introducing Boolean (Logical) Indexing\n",
    "\n",
    "In this exercise, you will filter rows using **conditions** (logical indexing), which we have not covered in detail in the lecture.\n",
    "\n",
    "1. Use `df.select_dtypes` to find at least one **numeric column**.\n",
    "2. Choose one numeric column, say `col = \"...\"`.\n",
    "3. Compute the median of this column.\n",
    "4. Create a **boolean mask** that is `True` for rows where the value in `col` is **greater than the median**.\n",
    "5. Use this mask to filter `df` so that you only keep rows where the value in `col` is above the median.\n",
    "\n",
    "*Hints:*\n",
    "- A boolean mask is a Series of True/False values, e.g.  \n",
    "  `mask = df[col] > df[col].median()`.\n",
    "- You can use it to filter the data frame: `df_filtered = df[mask]`.\n",
    "- Check `df_filtered.head()` to see whether the filtering worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174eb2ed",
   "metadata": {},
   "source": [
    "### Exercise 8 – Combining Multiple Conditions (Logical AND / OR)\n",
    "\n",
    "Now extend boolean indexing to use **two conditions**.\n",
    "\n",
    "1. Again, pick two numeric columns, for example `col1` and `col2`.\n",
    "2. Create a mask for `col1` being **above its median**.\n",
    "3. Create a mask for `col2` being **below its 75% quantile**.\n",
    "4. Combine these two masks using a **logical AND** (`&`) so that only rows satisfying *both* conditions remain.\n",
    "5. Create another combined mask using a **logical OR** (`|`) so that rows satisfying *at least one* of the two conditions remain.\n",
    "6. For each case, filter `df` and check how many rows you get.\n",
    "\n",
    "*Hints:*\n",
    "- Remember: parentheses are important!  \n",
    "  `mask = (df[col1] > df[col1].median()) & (df[col2] < df[col2].quantile(0.75))`\n",
    "- Use `.shape` or `len(df_filtered)` to see how many rows are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df237e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868bb93",
   "metadata": {},
   "source": [
    "### Exercise 9 – Creating New Columns Using Indexing\n",
    "\n",
    "1. Choose a numeric column that represents some kind of **price** or quantity.\n",
    "2. Compute its median and store it in a variable, e.g. `price_median`.\n",
    "3. Create a new boolean column `is_expensive` that is `True` if the price is above the median and `False` otherwise.\n",
    "4. Verify that the new column has been added by displaying the first 10 rows of `df` with `df.head(10)`.\n",
    "5. Use boolean indexing with this new column to create a subset `df_expensive` that only contains \"expensive\" rows.\n",
    "\n",
    "*Hints:*\n",
    "- You can assign new columns like this:  \n",
    "  `df[\"new_column\"] = ...`\n",
    "- Boolean columns work perfectly as masks:  \n",
    "  `df_expensive = df[df[\"is_expensive\"]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed32650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0602c",
   "metadata": {},
   "source": [
    "### Exercise 10 – Using `.loc` to Update Values Conditionally\n",
    "\n",
    "1. Reuse the `is_expensive` column from the previous exercise.\n",
    "2. Create another new column, e.g. `category`, and initialize it with the string `\"normal\"` for all rows.\n",
    "3. Using `.loc` and a boolean condition, set `category` to `\"expensive\"` for all rows where `is_expensive` is `True`.\n",
    "4. Check a few rows of `df` to confirm that the update worked.\n",
    "5. (Optional) Count how many rows fall into each category using `value_counts()`.\n",
    "\n",
    "*Hints:*\n",
    "- Pattern for conditional update:  \n",
    "  `df.loc[condition, \"column_name\"] = new_value`\n",
    "- Example (adapt to your column names):  \n",
    "  `df.loc[df[\"is_expensive\"], \"category\"] = \"expensive\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to the exercise task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ef453-9a75-4d60-b72f-b899d309c58f",
   "metadata": {},
   "source": [
    "# Data visualisation\n",
    "\n",
    "In this subsection, we take a graphical approach to understand our Resale Price Prediction dataset. For this purpose we first load the two most prominent libraries for data visualization – `Matplotlib` and `Seaborn`.\n",
    "\n",
    "**Matplotlib**: A foundational plotting library, Matplotlib is the granddaddy of Python visualization tools. It offers immense flexibility and allows us to create a wide variety of charts and plots with fine-grained control over every aspect of the visuals. Whether it's histograms, scatter plots, or line charts, Matplotlib provides the functionalities to craft them all with detailed customizations.\n",
    "\n",
    "**Seaborn**: Built on top of Matplotlib, Seaborn simplifies many visual tasks, making sophisticated plots accessible and understandable. It comes with built-in themes and color palettes that enhance the aesthetics of our visualizations. Seaborn is particularly adept at handling statistical graphics, making it easier to visualize complex datasets with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fe3ad-bb31-49a4-8e6f-3568e69c4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ignore future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82b121",
   "metadata": {},
   "source": [
    "## Visualizations for individual columns\n",
    "Visualizations of individual table columns depend on the type of data stored in that columns. We distinguish between numerical and categorical data. For numerical columns, it is common practice to analyze the distribution of the variable using *histograms* or *boxplots*.\n",
    "\n",
    "A *histogram* displays the distribution of a numerical variable by dividing the data into bins and counting the number of observations in each bin. This helps us understand the spread and central tendency of the data.\n",
    "\n",
    "A *boxplot*, on the other hand, provides a summary of the distribution of a numerical variable through its quartiles. It highlights the median, interquartile range, and potential outliers in the data.\n",
    "\n",
    "For categorical columns, *bar charts* are often used to visualize the frequency of each category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91c858",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "The following code creates a histogram for the column `Observed resale price`, which contains numerical data representing the resale prices of laptops. \n",
    "```python\n",
    "# Plot histogram for the 'Observed resale_price' column\n",
    "plt.hist(data['Observed resale price'], edgecolor='black')\n",
    "plt.title('Histogram of Observed Resale Price')\n",
    "plt.xlabel('Resale Price')  \n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c00e8f-b400-408a-b1f0-57bf9c57bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw on the above example to create a histogram for the \n",
    "# 'Observed resale price' column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4068ac0",
   "metadata": {},
   "source": [
    "Above, we use the library `Matplotlib`, which you can think of as a\n",
    "standard library for visualization. It offers much flexibility and facilitates creating various types of plots.\n",
    "\n",
    "In the scope of *data science* or *machine learning*, we routinely use a set of standard visualizations for data analysis, including the histogram. The library `seaborn` provides access to many of these plots through an easy-to-use interface. For example, creating the same histogram as before using `seaborn`, we would only need the following code: \n",
    "```python\n",
    "# Plot histogram using seaborn  \n",
    "sns.histplot(data=data, x='Observed resale price')\n",
    "plt.title('Histogram of Observed Resale Price')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also try out the seaborn version of the histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf054920",
   "metadata": {},
   "source": [
    "### Boxplot\n",
    "The creation of a boxplot follows a similar structure as the histogram. Using `Matplotlib`, we can create a boxplot for the `Observed resale price` column with the following code:\n",
    "```python   \n",
    "plt.boxplot(data['Observed resale price'], vert=True, patch_artist=True)\n",
    "plt.title('Boxplot of Observed Resale Price')   \n",
    "plt.ylabel('Resale Price')\n",
    "plt.show()\n",
    "```\n",
    "Alternatively, we can use `seaborn` to create the same boxplot with less code:\n",
    "```python   \n",
    "sns.boxplot(data=data, x='Observed resale price')\n",
    "plt.title('Boxplot of Observed Resale Price')\n",
    "plt.show()\n",
    "```\n",
    "Considering the `seaborn` example, note how we specify the data frame and the column name directly within the `sns.boxplot()` function. More specifically, we use the function argument `x` to indicate that we want to plot the values of the `Observed resale price` column on the x-axis. To rotate the boxplot to be vertical instead of horizontal, you can exchange the `x` and use the argument `y` instead. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a boxplot for the 'Observed resale price' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e890a",
   "metadata": {},
   "source": [
    "### Countplot\n",
    "A *countplot* is a type of bar chart that displays the frequency of each category in a categorical variable. It is particularly useful for visualizing the distribution of categorical data and identifying the most common categories. Here's how you can create a countplot for a categorical column, such as `Brand`, using our two plotting libraries:\n",
    "```python\n",
    "# Using Matplotlib\n",
    "brand_counts = data['Brand'].value_counts()\n",
    "plt.bar(brand_counts.index, brand_counts.values)\n",
    "plt.title('Countplot of Brand')\n",
    "plt.xlabel('Brand')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "``` \n",
    "```python   \n",
    "# Using Seaborn\n",
    "sns.countplot(data=data, x='Brand') \n",
    "plt.title('Countplot of Brand')\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63faa5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create countplot for the 'Brand' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d4c2d",
   "metadata": {},
   "source": [
    "\n",
    "These examples hint at the advantages of using `seaborn` for standard visualizations, as it often requires less code and provides a more straightforward interface for creating common plots. For example, the `sns.countplot()` function directly handles the counting of categories, while in the `Matplotlib` example, we first need to compute the counts using `value_counts()` before plotting. We could emphasize this point further by creating a *grouped countplot*. With our data, we could, for example, examine the frequency of different laptop brands (column `Brand`) across levels of another variable, such as, `Release year`. This is easily done using `seaborn`, which supports an argument `hue` for grouping purposes. Creating the same kind of visualization using `Matplotlib` requires a lot more code and is, therefore, not demonstrated here. \n",
    "```python\n",
    "# Using Seaborn to create a stacked countplot\n",
    "sns.countplot(data=data, x='Brand', hue='Release year')\n",
    "plt.title('Stacked Countplot of Brand by Release year')\n",
    "plt.show()\n",
    "```\n",
    "> Hint: if you try out the above code, you may notice that the `Brand` column has many unique values. This can lead to a cluttered and hard-to-read plot. To improve readability, you could use logical indexing to first create a new dataframe containing only a handful of brands and to then create the grouped plot for this filtered dataframe. Exercise 7 and 8 on Pandas indexing provide the necessary background.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd040467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a grouped countplot for the 'Brand' column across 'Release years'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714452dd",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The previous examples have illustrated standard visualizations for individual columns. Commonly, we need to examine all columns in a dataset. From a programming perspective, we could achieve this by using loops. Here is your exercise task:\n",
    "1. Write a `for` loop that iterates over all columns of the dataset. You can access the column names using `data.columns`\n",
    "2. Inside the loop, check the data type of the current column. Note the hints belows on how to do this\n",
    "  a) If the column is of numerical type, plot its distribution using a histogram\n",
    "  b) If the column is of categorical type, plot its distribution using a countplot\n",
    "\n",
    "*Hints:*\n",
    "- We suggest you use the `seaborn` library for creating the plots, as it requires less code. However, the choice is yours.\n",
    "- To check a column's data type, you can use `data[column_name].dtype`.\n",
    "- Numerical data types typically include `int64` and `float64`, while categorical data types are often represented as `object` or `category`. \n",
    "- Hence, you can use the statement  `data[column_name].dtype in ['int64', 'float64']` to check for numerical columns.\n",
    "- Likewise, you can check for categorical columns with `data[column_name].dtype in ['object', 'category']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the visualization exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d489ab7-4e54-4ed5-a272-b3af9e208ac5",
   "metadata": {},
   "source": [
    "# Visualizations for relationships between columns\n",
    "Strictly speaking, the previous example of a grouped countplot already visualizes relationships between two columns – *Brand* and *Release year*. In this section, we will focus more explicitly on visualizations that analyze relationships between multiple columns. In this scope, we begin with the most obvious example of a scatter plot, showing the association between two numerical columns. Next, we consider correlation, a measure summarizing the linear association between two columns in a single number. We will compute a *correlation matrix*, which consists of the pairwise correlations of all numerical columns in the data. Lastly, we exemplify the violin plot, a less common but powerful way to analyze the distribution of a numerical variable across differt levels of up to two categorical variables. \n",
    "\n",
    "\n",
    "## Scatter Plot\n",
    "A *scatter plot* is a graphical representation that displays the relationship between two numerical variables. Each point on the scatter plot represents an observation in the dataset, with its position determined by the values of the two variables being compared. Scatter plots are particularly useful for identifying patterns, trends, and potential correlations between the variables. We can create scatter plots using both, `Matplotlib` and `Seaborn`. Below, we provide examples for both libraries, visualizing the relationship between the columns `Observed resale price` and `Original price`.\n",
    "```python\n",
    "# Using Matplotlib to create a scatter plot\n",
    "plt.scatter(data['Retail price'], data['Observed resale price'])\n",
    "plt.title('Scatter Plot of Observed Resale Price vs original Retail Price')\n",
    "plt.xlabel('Original Price')\n",
    "plt.ylabel('Observed Resale Price')\n",
    "plt.show()\n",
    "``` \n",
    "```python\n",
    "# Using Seaborn to create a scatter plot\n",
    "sns.scatterplot(data=data, x='Retail price', y='Observed resale price')\n",
    "plt.title('Scatter Plot of Observed Resale Price vs Original Price')\n",
    "plt.show()      \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to a scatter plot showing the relationship between 'Original price' and 'Observed resale price' or other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51971a16",
   "metadata": {},
   "source": [
    "Our choice of the two columns `Retail price` and `Observed resale price` is natural given the context of the dataset, reselling used items in the second-hand market. However, we may wish to also examine the association between other columns. Speaking about other columns, it would be useful to obtain a visualization of *all the main patterns* in the data with a single function call. While no function can - or should - promise to reveal *all main patters*, the `pairplot()` function from `seaborn` is a useful tool for data exploration and obtaining some first-level insights into a dataset. It creates a grid of scatter plots for each pair of numerical variables, visualizing their pairwise relationships. Additionally, it includes histograms or density plots along the diagonal to show the distribution of each individual variable. With these characteristics, `pairplot()` provides a comprehensive visualization of the numerical columns of a dataset and is very easy to use. \n",
    ">Note that creating a `pairplot` (i.e., executing the next cell) will take a bit of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab893cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the entire dataset with pairplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c693a8",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "Next, we consider the correlation between table columns. Recall that *correlation* is a measure for much two numerical random variables (e.g., table columns) are linearly related. To compute the pairwise correlation between all table columns, we can use the `corr()` function, which Pandas provides. Afterwards, we can visualize all the pairwise correlations as a headmap for easy inspection. To achieve this, we will use the `heatmap` function from `seaborn`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Compute Pearson correlation between all numerical columns of the dataset\n",
    "rho = data.corr(method='pearson', numeric_only=True)  # we must explicitly exclude non-numeric columns\n",
    "# heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(data=rho, cmap='coolwarm', fmt=\".2f\", linecolor='black', linewidth=0.5)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef91c93",
   "metadata": {},
   "source": [
    "## Violin Plots\n",
    "Violin plots are a combination of box plots and kernel density plots. They provide a more detailed view of the distribution of numerical features across different categories. Arguably, this type of visualization is less common. However, it can be useful and so we complete this part with a few demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb64672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot of the feature LOAN grouped by the feature JOB\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=data, x='Brand', y='Retail price', inner='box') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677d9d8",
   "metadata": {},
   "source": [
    "Above, we promised that the violin plot can handle up to two categorical variables. To demonstrate this, we first need a new categorical variable with no more than two levels, that is a dummy variable. We create a dummy variable indicating whether a notebook's resale price is above its median value. Then, the following code creates a violin plot for retail prices, grouped by the categorical column *Brand*, where we split each \"violin\" into two halfs, showing the retail price distribution for notebooks with resale prices above and below their median, respectively. Do not think too much into this visualization. It's main purpose is to showcase a violin plot with 3 dimensions. \n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy column to the dataset (needed for our 3D visualization plot)\n",
    "data[\"AboveMedian\"] = data[\"Observed resale price\"] > data[\"Observed resale price\"].median()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(\n",
    "    data=data,\n",
    "    x='Brand',\n",
    "    y='Retail price',\n",
    "    hue='AboveMedian',\n",
    "    split=True,\n",
    "    inner='box'\n",
    ")\n",
    "plt.title(\"Violin Plot of Retail Price by Brand (Above vs. Below Median)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
