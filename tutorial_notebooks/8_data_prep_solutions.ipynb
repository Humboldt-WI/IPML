{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iGz0qnmS-UK"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/ipml/blob/master/tutorial_notebooks/8_data_prep_solutions.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHScrtl_S-UK"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "This is the second notebook in a series revisiting our lecture on EDA and data preparation. Having elaborated on the `Pandas` library and having illustrated the use of standard plotting libraries in [Tutorial 7](), we turn our attention to the data preparation part of the lecture. In this scope, we investigate options to scale and categorize continuous features and properly encode categorical features before using them in a machine learning model. We will see that most of the needed functionality is readily available in `Pandas`.  \n",
    "\n",
    "We continue to use our credit risk data set for this tutorial. Before moving on, we perform some standard setup tasks, which, by now, you know well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33ZdGrYCS-UK"
   },
   "outputs": [],
   "source": [
    "# Load standard libraries\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load credit risk data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "hmeq = pd.read_csv(data_url)\n",
    "\n",
    "# Preview the data\n",
    "hmeq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZL4c5HyS-UU"
   },
   "source": [
    "# Altering Data Types\n",
    "We start with a rather technical bit, data types. Recall the way our data is stored at the moment. \n",
    "The features *JOB* and *REASON* are stored as data type `object`. This is the most general data type in Python. A variable of this type can store pretty much any piece of data, numbers, text, dates, times, ... This generality has a price. First, storing data as data type `object` consumes a lot of memory. Second, we cannot access specific functionality that is available for a specific data type only. Functions to manipulate text are an example. These are available for data of type `string` but not for data of type `object`. \n",
    "<br>\n",
    "In our case, the two features that Pandas stores as objects are actually categorical variables. We can easily verify this using, e.g., the `value_counts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbBgoi3uS-UU",
    "outputId": "3fc66b34-74e2-472b-a185-0c929b7edc33"
   },
   "outputs": [],
   "source": [
    "print(hmeq.REASON.value_counts())  # so REASON is a binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3FeAT1dS-UU",
    "outputId": "382ae29c-898f-4505-f9b2-7ddd148bb057"
   },
   "outputs": [],
   "source": [
    "print(hmeq.JOB.value_counts())  # JOB is a categorical variable with many levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4JsilV3S-UU"
   },
   "source": [
    "Knowing our two \"object features\" are categories, we should alter their data type accordingly. To that end, we make use of the function `astype`, which facilitates converting one data type into another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MO6IxmTS-UU",
    "outputId": "8f52d60c-034d-4ecb-9fad-873995bbc710"
   },
   "outputs": [],
   "source": [
    "# Code categories properly \n",
    "hmeq['REASON'] = hmeq['REASON'].astype('category')\n",
    "hmeq['JOB'] = hmeq['JOB'].astype('category')\n",
    "hmeq.info()  # verify the conversion was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXA9BTp_S-UV"
   },
   "source": [
    "Although it does not really matter for this tiny data set, note that the conversion from object to category has reduced the amount of memory that the data frame consumes. On my machine, we need 524.2 KB after the translation, whereas we needed more than 600 KB for the original data frame. If you work with millions of observations the above conversion can result in a significant reduction of memory consumption. If memory consumption is an issue, we can achieve a significant further reduction by reducing the precision of the numerical variables. *Downcasting* from float64 to float32 is likely ok for predictive modeling. Also, the target variable is stored as an integer but we know that it has only two states. Thus, we can convert the target to a boolean. We perform these transformations in the next code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-K5ln75FS-UV"
   },
   "outputs": [],
   "source": [
    "# The target variable has only two states so that we can store it as a boolean\n",
    "hmeq['BAD'] = hmeq['BAD'].astype('bool')\n",
    "\n",
    "# For simplicity, we also convert LOAN to a float so that all numeric variables are of type float\n",
    "hmeq['LOAN'] = hmeq['LOAN'].astype(np.float64)\n",
    "\n",
    "# Last, let's change all numeric variables from float64 to float32 to reduce memory consumption\n",
    "num_vars = hmeq.select_dtypes(include=np.float64).columns\n",
    "hmeq[num_vars] = hmeq[num_vars].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhqJ1Ty0S-UV"
   },
   "source": [
    "Invest some time to understand the above codes. Our examples start to combine multiple pieces of functionality. For example, the above demo uses indexing, functions, and function arguments to perform tasks. Keep practicing and you will become familiar with the syntax.\n",
    "<br>\n",
    "Finally, let's verify our changes once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cuBwfDSS-UV",
    "outputId": "e07680f7-dac9-423e-aa82-ee14b05f690f"
   },
   "outputs": [],
   "source": [
    "# Check memory consumption after the conversions\n",
    "hmeq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8t60vNRS-UV"
   },
   "source": [
    "In total, our type conversions reduced memory consumption by more than a half. You might want to bear this potential in mind when using your computer to process larger data sets. Should you be interested in some more information on memory efficiency, have a look at this post at [TowardDataScience.com](https://towardsdatascience.com/pandas-save-memory-with-these-simple-tricks-943841f8c32). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ypE7GuOS-UW"
   },
   "source": [
    "# Missing values\n",
    "Our data contains many missing values. This is easily seen when calling, e.g., `hmeq.head(5)`, and is common when working with real data. Likewise, handling missing values is a standard task in data preparation. `Pandas` provides the function `.isna()` as entry point to the corresponding functionality and helps with identifying the relevant cases.\n",
    "\n",
    "*Note*: `Pandas` also supports an equivalent function called `.isnull()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeahGMfES-UW",
    "outputId": "9141fa4f-01b3-4f3f-fa63-5e744bcb1c5f"
   },
   "outputs": [],
   "source": [
    "# Boolean mask of same size as the data frame to access missing values via indexing\n",
    "missing_mask = hmeq.isna()\n",
    "\n",
    "print(f'Dimension of the mask: {missing_mask.shape}')\n",
    "print(f'Dimension of the data frame: {hmeq.shape}')\n",
    "\n",
    "missing_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aBde7TBS-UW"
   },
   "source": [
    "We can now count the number of missing values per row or per column or in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_DgQ7QHS-UW",
    "outputId": "b931d0ae-fece-4d25-d8d9-0a352b4da582"
   },
   "outputs": [],
   "source": [
    "# missing values per row\n",
    "miss_per_row = missing_mask.sum(axis=1)\n",
    "print('Missing values per row:\\n', miss_per_row)\n",
    "print('*' * 50)\n",
    "# missing values per column\n",
    "miss_per_col = missing_mask.sum(axis=0)\n",
    "print('Missing values per column:\\n', miss_per_col )\n",
    "print('*' * 50)\n",
    "# count the total number of missing values\n",
    "n_total_missing = missing_mask.sum().sum()\n",
    "print(f'Total number of missing values: {n_total_missing}')\n",
    "print('*' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_9jyQY_S-UW"
   },
   "source": [
    "It can be useful to visualize the *missingness* in a data set by means of a heatmap. Note how the below example gives you a good intuition of how and where the data set is affected by missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggtNOCJKS-UW",
    "outputId": "c66c163d-4ae4-46ec-978a-49cca891d75d"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(hmeq.isna())  # quick visualization of the missing values in our data set\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnw8ySRgS-UW"
   },
   "source": [
    "## Categorical features\n",
    "Let's start with the two categorical features, `REASON` and `JOB`. We will treat them differently for the sake of illustration. Now that we start altering our data frame more seriously, it is a good idea to make a copy of the data so that we can easily go back to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cuSqLNgzS-UW"
   },
   "outputs": [],
   "source": [
    "# copy data: we continue with altering the variable df while we keep variable hmeq for the raw data\n",
    "df = hmeq.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7nqgRZuS-UX"
   },
   "source": [
    "### Adding a new category level\n",
    "One way to treat missing values in a categorical feature is to introduce a new category level \"IsMissing\". We will demonstrate this approach for the feature *REASON*. \n",
    "<br>One feature of the category data type in Pandas is that category levels are managed. We cannot add levels directly. Thus, before assigning the missing values our new category level *IsMissing*, we first need to introduce this level. We basically tell our data frame that *IsMissing* is another suitable entry for *REASON* next to the levels that already exist in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdBWbWtfS-UX",
    "outputId": "b0338de2-ff64-4ff7-d922-1142f7f92879"
   },
   "outputs": [],
   "source": [
    "# Variable REASON: we treat missing values as a new category level.\n",
    "# First we need to add a new level\n",
    "df.REASON = df.REASON.cat.add_categories(['IsMissing'])\n",
    "\n",
    "# Now we can do the replacement\n",
    "df.REASON[df.REASON.isna() ] = \"IsMissing\"\n",
    "df.REASON.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXAjFQFtS-UX",
    "outputId": "b5d3bf61-c04e-4d4f-a5f0-d345f8d3b184"
   },
   "outputs": [],
   "source": [
    "df.REASON.isna().sum()  # verify that no more missing values exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf_LOVmPS-UX"
   },
   "source": [
    "### Mode replacement\n",
    "For the feature *JOB*, which is multinomial, we replace missing values with the mode of the feature. Please note that this is a crude way to handle missing values. I'm not endorsing it! But you should have at least seen a demo. Here it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNUN63yS-UX",
    "outputId": "a5ecc720-55f7-48b2-967a-27b387f50ad1"
   },
   "outputs": [],
   "source": [
    "# Determine the mode\n",
    "mode_of_job = df.JOB.mode()\n",
    "print(mode_of_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M32XJxQS-UY",
    "outputId": "a6d86590-c2d8-4716-adcf-9fa8e258b3ac"
   },
   "outputs": [],
   "source": [
    "# replace missing values with the mode\n",
    "df.JOB[df.JOB.isna() ] = df.JOB.mode()[0]  # the index [0] is necessary as the result of calling mode() is a Pandas Series\n",
    "# verify that no more missing values exist\n",
    "df.JOB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4SvhbFnS-UY",
    "outputId": "ecb4b0ef-ca74-4890-cc24-0b46e0738747"
   },
   "outputs": [],
   "source": [
    "# Verify more seriously that missing value replacement was successful\n",
    "if df.REASON.isna().any() == False and df.JOB.isna().any() == False:\n",
    "    print('well done!')\n",
    "else:\n",
    "    print('ups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZaNFpGS-UZ"
   },
   "source": [
    "## Numerical features\n",
    "We have a lot of numerical features. To keep things simple, we simply replace all missing values with the median. Again, this is  a crude approach that should be applied with care; if at all. However, it nicely shows how we can process several columns at once using a loop. Further, note the use of the method `fillna()` to replace missing values. We could have also used it in our previous examples concerning categorical features. It is the recommend way to replace missing values in `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kjlpGozNS-UZ"
   },
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='float32').columns:  # loop over all numeric columns\n",
    "    if df[col].isna().sum() > 0:                         # check if there are any missing values in the current feature\n",
    "        m = df[col].median(skipna=True)                  # compute the median of that feature\n",
    "        df[col].fillna(m, inplace=True)                  # replace missing values with the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you wonder whether it is necessary to write a loop to perform this rather standard operation, the answer is no. You could achieve the same result more elegantly when combining the `fillna()` method with a call to the method `transform()`. Here is how this would look like:\n",
    "```python\n",
    "# Alternative approach to impute missing values with the feature median\n",
    "cols = df.select_dtypes(include='float32').columns \n",
    "\n",
    "df[cols] = df[cols].transform(lambda x: x.fillna(x.median()))\n",
    "``` \n",
    "The function `transform()` applies a function to each column of the DataFrame. The lambda function takes each column, fills the missing values with the median of that column, and returns the transformed column. This way, you avoid looping over each column manually. This version can be considered more elegant, but our first shot, writing a loop, may legitimately be considered more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukR_VzGS-UZ",
    "outputId": "13f49cee-b983-4e17-ddb3-ebef2477dff7"
   },
   "outputs": [],
   "source": [
    "# Verify there are no more missing values in the data\n",
    "n_total_missing = df.isna().sum().sum()\n",
    "if  n_total_missing == 0:\n",
    "    print('Well done, no more missing values!')\n",
    "else:\n",
    "    print(f'Ups! There are still {n_total_missing} missing values.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## 1. Outliers\n",
    "The lecture introduced a rule of thumb saying that, for a given feature, a feature value $x$ can be considered an outlier if \n",
    "$$x >q_3(X) + 1.5 \\cdot IQR(X)$$\n",
    "\n",
    "where $q_3(X)$ denotes the third quantile of the distribution of feature $X$ and $IQR(X)$ the corresponding inter-quartile range.\n",
    "\n",
    "1. Use the `Pandas` method `quantile` to compute the third and first quartile of feature `LOAN`.\n",
    "2. Compute the threshold value that a feature value $x$ must not exceed according to the above equation. Store the result in a variable. \n",
    "3. Use logical indexing to identify all upper outliers in the feature `LOAN`.\n",
    "4. Create a new data frame that has no outliers in the feature `LOAN`. To that end: \n",
    "- Reuse your solution to task 3 to identify outliers using indexing\n",
    "- Change the `LOAN` values for all outlier cases to the threshold you computed in step 2.\n",
    "\n",
    "Follow-up, more advanced tasks:<br>\n",
    "\n",
    "5. Write a custom function that implements the functionality you created in task 4. Make the feature to work on an argument of your function.\n",
    "6. Call your custom function for all numerical features in the data frame. The goal is to create a data frame that does not have any upper outlier in any of its numerical features. To demonstrate the capabilities of your function, set the threshold to $3 \\cdot IQR(X)$. This way, only extreme outliers will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First and third quantile of the LOAN feature\n",
    "quantiles = df[\"LOAN\"].quantile(q=[0.25, 0.75])\n",
    "\n",
    "# To extract the actual numbers into easy-to-use variables,\n",
    "# we can first create a tuple and then use unpacking\n",
    "q1, q3 = (quantiles.values)\n",
    "print(f\"The first and third quartile are, respectively {q1} and {q3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Threshold value for upper outlier\n",
    "tau = q3 + 1.5*(q3-q1)\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find upper outliers in the LOAN feature\n",
    "ix_upper_outlier = df[\"LOAN\"]>tau \n",
    "df.loc[ix_upper_outlier, \"LOAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove upper outliers in feature LOAN\n",
    "df.loc[ix_upper_outlier, \"LOAN\"] = tau  # outlier truncation\n",
    "df.loc[ix_upper_outlier, \"LOAN\"]  # print results to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Customer function for outlier detection and removal \n",
    "def outlier_truncation(x, factor=1.5):\n",
    "    \"\"\"\n",
    "    Identifies outlier values based on the inter-quartile range IQR. \n",
    "    Corresponding outliers are truncated and set to a contant value equal to the IQR\n",
    "    times a factor, which, following Tuckey's rule, we set to 1.5 by default\n",
    "    \n",
    "        Parameters:\n",
    "            x (Pandas Series): A data frame column to scan for outliers\n",
    "            factor (float): An outlier is a value this many times the IQR above q3/below q1\n",
    "            \n",
    "        Returns:\n",
    "            Adjusted variable in which outliers are truncated\n",
    "    \"\"\"\n",
    "    x_new = x.copy()\n",
    "    \n",
    "    # Calculate IQR\n",
    "    IQR = x.quantile(0.75) - x.quantile(0.25) \n",
    "    \n",
    "    # Define upper/lower bound\n",
    "    upper = x.quantile(0.75) + factor*IQR\n",
    "    lower = x.quantile(0.25) - factor*IQR\n",
    "    \n",
    "    # Truncation\n",
    "    x_new[x < lower] = lower.astype(np.float32)  # downcasting to float32 is needed to ensure\n",
    "    x_new[x > upper] = upper.astype(np.float32)  # compatibility with how we store the data in our data frame \n",
    "    \n",
    "    return x_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Application of the function to all numerical features in the data \n",
    "\n",
    "# Select numeric variables for outlier treatment. \n",
    "ix_numerical = df.select_dtypes(include=\"float32\").columns\n",
    "\n",
    "# Process every selected column using apply\n",
    "# Updated 10.06.20 to show passing arguments to the 'applied' functions. Just send a tuple with arguments in the order as specified\n",
    "# by the called function leaving out the first argument (see, https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html)\n",
    "df[ix_numerical] = df[ix_numerical].apply(outlier_truncation, axis=0, factor=3)  \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling numerical features\n",
    "Another common data preparation task is scaling numerical features. The goal is to ensure that all features have the same scale. This is important for many machine learning algorithms. The lecture introduced two common scaling methods: min-max scaling and z-score scaling.\n",
    "The `sklearn` library provides implementations of both approaches in the classes `MinMaxScaler` and `StandardScaler`, which are part of the module `preprocessing`. Experiment with these classes to solve the following exercises.\n",
    "\n",
    "1. Import the class `MinMaxScaler` and `StandardScaler` from the module `preprocessing` in the library `sklearn`.\n",
    "2. Familiarize yourself with the functioning of the `StandardScaler` using its documentation and other sources (e.g., web search). \n",
    "3. Test the `StandardScaler` by applying it to the numerical features `LOAN`. Afterwards, the scaled feature should have a mean of 0 and a standard deviation of 1. Write a few lines of code to verify this.\n",
    "4. The use of the `MinMaxScaler` is similar to the `StandardScaler`. Apply the `MinMaxScaler` to all other numerical features in the data set. More specifically, \n",
    "- Create a new data frame that contains only the numerical features.\n",
    "- Remove the feature `LOAN` from that data frame; as we already scaled it in task 3.\n",
    "- Apply the `MinMaxScaler` to the new data frame.\n",
    "- Write a few lines of code to verify that the scaling was successful. To that end, recall what the 'MinMaxScaler' does.\n",
    "- Combine the scaled features with the feature `LOAN` and the categorical features in a new `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the class MinMaxScaler and StandardScaler from the module preprocessing in the library sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# 2. Familiarize yourself with the functioning of the StandardScaler using its documentation and other sources (e.g., web search).\n",
    "# For example, you could start here: https://scikit-learn.org/1.5/api/sklearn.preprocessing.html\n",
    "\n",
    "# 3. Test the StandardScaler by applying it to the numerical feature LOAN. Afterwards, the scaled feature should have a mean of 0 and a standard deviation of 1. Write a few lines of code to verify this.\n",
    "scaler = StandardScaler()\n",
    "df[\"LOAN_scaled\"] = scaler.fit_transform(df[[\"LOAN\"]])\n",
    "\n",
    "# Verify the scaling\n",
    "print(f\"Mean of LOAN_scaled: {df['LOAN_scaled'].mean()}\")\n",
    "print(f\"Standard deviation of LOAN_scaled: {df['LOAN_scaled'].std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Apply the MinMaxScaler to all other numerical features in the data set.\n",
    "# Create a new data frame that contains only the numerical features.\n",
    "df_numerical = df.select_dtypes(include=\"float32\").copy()\n",
    "\n",
    "# Remove the feature LOAN from that data frame; as we already scaled it in task 3.\n",
    "df_numerical.drop(columns=[\"LOAN\"], inplace=True)\n",
    "\n",
    "# Apply the MinMaxScaler to the new data frame.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_numerical_scaled = pd.DataFrame(min_max_scaler.fit_transform(df_numerical), columns=df_numerical.columns)\n",
    "\n",
    "# Verify the scaling\n",
    "print(f\"Min values of scaled features:\\n{df_numerical_scaled.min()}\")\n",
    "print(f\"Max values of scaled features:\\n{df_numerical_scaled.max()}\")\n",
    "\n",
    "# Combine the scaled features with the feature LOAN and the categorical features in a new DataFrame.\n",
    "df_scaled = pd.concat([df[[\"LOAN_scaled\"]], df_numerical_scaled, df.select_dtypes(include=[\"category\", \"bool\"])], axis=1)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discretizing numerical features\n",
    "Discretizing numerical features is another common data preparation task. The goal is to convert continuous numerical features into discrete bins or categories. This can be useful for certain types of analysis and modeling. The `pandas` library provides the `cut` and `qcut` functions for this purpose.\n",
    "\n",
    "1. Familiarize yourself with the `cut` and `qcut` functions in the `pandas` library using their documentation and other sources (e.g., web search).\n",
    "2. Use the `cut` function to discretize the `LOAN` feature into 5 equal-width bins. Assign meaningful labels to each bin (e.g., 'Very Low', 'Low', 'Medium', 'High', 'Very High').\n",
    "3. Verify the binning by displaying the first few rows of the data frame and checking the `LOAN` feature.\n",
    "4. Use the `qcut` function to discretize the `MORTDUE` feature into 4 quantile-based bins. Assign meaningful labels to each bin (e.g., 'Q1', 'Q2', 'Q3', 'Q4').\n",
    "5. Verify the binning by displaying the first few rows of the data frame and checking the `MORTDUE` feature.\n",
    "\n",
    "Follow-up, more advanced tasks:<br>\n",
    "\n",
    "6. Create a new data frame that includes the discretized `LOAN` and `MORTDUE` features along with the other original features.\n",
    "7. Write a custom function that takes a data frame and a list of numerical features as input and returns a new data frame with all specified features discretized into a given number of bins using the `cut` function. Test your function on the numerical features in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use the cut function to discretize the LOAN feature into 5 equal-width bins. Assign meaningful labels to each bin (e.g., 'Very Low', 'Low', 'Medium', 'High', 'Very High').\n",
    "loan_bins = pd.cut(df[\"LOAN\"], bins=5, labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "df[\"LOAN_bins\"] = loan_bins  # We add a new column as opposed to overwriting the existing column\n",
    "\n",
    "# 3. Verify the binning by displaying the first few rows of the data frame and checking the LOAN feature.\n",
    "print(df[[\"LOAN\", \"LOAN_bins\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the qcut function to discretize the MORTDUE feature into 4 quantile-based bins. Assign meaningful labels to each bin (e.g., \"Q1\", \"Q2\", \"Q3\", \"Q4\").\n",
    "mortdue_bins = pd.qcut(df[\"MORTDUE\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "# 5. Verify the binning by displaying the first few rows of the data frame and checking the MORTDUE feature.\n",
    "df[\"MORTDUE_bins\"] = mortdue_bins\n",
    "print(df[[\"MORTDUE\", \"MORTDUE_bins\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a new data frame that includes the discretized LOAN and MORTDUE features along with the other original features.\n",
    "df_discretized = df.drop(columns=[\"LOAN\", \"MORTDUE\"])\n",
    "df_discretized = pd.concat([df_discretized, loan_bins, mortdue_bins], axis=1)\n",
    "df_discretized  # preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Write a custom function that takes a data frame and a list of numerical features as input and returns a new data frame with all specified features discretized into a given number of bins using the cut function. Test your function on the numerical features in the data frame.\n",
    "def discretize_features(df, features, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Discretizes the specified columns of a DataFrame into equal-width bins.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data to be discretized.\n",
    "    columns (list of str): The list of column names to be discretized.\n",
    "    bins (int): The number of equal-width bins to use for discretization.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new DataFrame with the specified columns discretized into bins.\n",
    "\n",
    "    \"\"\"\n",
    "    df_discretized = df.copy()\n",
    "    for feature in features:\n",
    "        df_discretized[feature + \"_bins\"] = pd.cut(df_discretized[feature], bins=bins, labels=labels)\n",
    "    return df_discretized\n",
    "\n",
    "# Test the function on the numerical features in the data frame\n",
    "ix_numerical = df.select_dtypes(include=\"float32\").columns\n",
    "df_discretized_all = discretize_features(df, ix_numerical, bins=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "df_discretized_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "Encoding categorical features is another important data preparation task. Machine learning algorithms require numerical input, so we need to convert categorical features into numerical format. The `pandas` library provides the `get_dummies` function for this purpose. This function will turn a categorical feature with k levels into k binary features, as shown in this example for a categorical feature COLOR:\n",
    "\n",
    "**Original Table**\n",
    "\n",
    "| ID | COLOR  |\n",
    "|----|--------|\n",
    "| 1  | Red    |\n",
    "| 2  | Blue   |\n",
    "| 3  | Green  |\n",
    "| 4  | Red    |\n",
    "| 5  | Green  |\n",
    "\n",
    "**Table After Dummy Coding**\n",
    "\n",
    "| ID | COLOR_Red | COLOR_Blue | COLOR_Green |\n",
    "|----|-----------|------------|-------------|\n",
    "| 1  | 1         | 0          | 0           |\n",
    "| 2  | 0         | 1          | 0           |\n",
    "| 3  | 0         | 0          | 1           |\n",
    "| 4  | 1         | 0          | 0           |\n",
    "| 5  | 0         | 0          | 1           |\n",
    "\n",
    "Let's move on to some exercises.\n",
    "\n",
    "\n",
    "1. Familiarize yourself with the `get_dummies` function in the `pandas` library using its documentation and other sources (e.g., web search).\n",
    "2. Apply the `get_dummies` function to discretize the `REASON` feature. Assign meaningful column names to the resulting dummy variables.\n",
    "3. Verify the result of the previous dummy-coding step. You should see that the `get_dummies` function has created three binary variables, one for each of the three levels of the *REASON* feature. Recalling the functioning of regression methods, you now face a problem. The three new dummy variables are **linearly dependent**. Regression models will not work with linearly dependent features. Redo task 3 but this time pay attention to the argument `drop_first`, which the `get_dummies` function offers. Set this argument in such a way that you avoid linear dependency among the created dummy variables. \n",
    "\n",
    "Additional, more advanced exercises:<br>\n",
    "\n",
    "4. This task assumes you solved exercise 7 from the above tasks concerning discretization. If you have not done so, go back to the discretization exercises and solve all tasks including task 7.\n",
    "5. Discretize all numerical features in the data frame using the custom function you created in task 7. Apply the `get_dummies` function to the discretized features in the resulting data frame while avoiding linear dependency. Next, create a data frame `df_all_dummy` that includes only the dummy-encoded features. \n",
    "6. Fit a logistic regression model to the `df_all_dummy` data frame. Use the class `LogisticRegression` from the module `sklearn.linear_model`. The handling of `LogisticRegression` is similar to the handling of `LinearRegression`, which we examined in [Tutorial 5](https://github.com/Humboldt-WI/IPML/blob/main/tutorial_notebooks/5_SML_for_regression_solutions.ipynb). Train the model using the `fit()` method. Then, print the estimated coefficients and compute the model's performance on the training set using the method `score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Familiarize yourself with the `get_dummies` function in the `pandas` library using its documentation and other sources (e.g., web search).\n",
    "help(pd.get_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply the `get_dummies` function to discretize the `REASON` feature. Assign meaningful column names to the resulting dummy variables.\n",
    "reason_dummies = pd.get_dummies(df[\"REASON\"], prefix=\"REASON\")\n",
    "reason_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Verify the result of the previous dummy-coding step. \n",
    "# As shown above, the `get_dummies` function has created three binary variables, one for each of the three levels of the *REASON* feature. Recalling the functioning of regression methods, you now face a problem. \n",
    "# The three new dummy variables are **linearly dependent**. Regression models will not work with linearly dependent features. Redo task 3 but this time pay attention to the argument `drop_first`, which the `get_dummies` function offers. Set this argument in such a way that you avoid linear dependency among the created dummy variables. \n",
    "reason_dummies = pd.get_dummies(df[\"REASON\"], prefix=\"REASON\", drop_first=True)\n",
    "reason_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. This task assumes you solved exercise 7 from the above tasks concerning discretization. If you have not done so, go back to the discretization exercises and solve all tasks including task 7.\n",
    "\n",
    "def discretize_features(df, features, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Discretizes the specified columns of a DataFrame into equal-width bins.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data to be discretized.\n",
    "    columns (list of str): The list of column names to be discretized.\n",
    "    bins (int): The number of equal-width bins to use for discretization.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new DataFrame with the specified columns discretized into bins.\n",
    "\n",
    "    \"\"\"\n",
    "    df_discretized = df.copy()\n",
    "    for feature in features:\n",
    "        df_discretized[feature + \"_bins\"] = pd.cut(df_discretized[feature], bins=bins, labels=labels)\n",
    "    return df_discretized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Discretize all numerical features in the data frame using the custom function you created in task 7. \n",
    "ix_numerical = df.select_dtypes(include=np.number).columns\n",
    "df_discretized_all = discretize_features(df, ix_numerical, bins=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Create a version of the data frame with only the discretized features\n",
    "df_only_discrete = df_discretized_all.select_dtypes(exclude=np.number)\n",
    "df_only_discrete.drop(columns=['REASON', 'JOB'], inplace=True)  # we remove the target variable\n",
    "\n",
    "# Apply the `get_dummies` function to the discretized features in the resulting data frame while avoiding linear dependency. Next, create a data frame `df_all_dummy` that includes only the dummy-encoded features.\n",
    "df_all_dummy = pd.get_dummies(df_only_discrete, drop_first=True)    # drop_first=True to avoid linear dependency\n",
    "df_all_dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6  Fit a logistic regression model to the `df_all_dummy` data frame and print the estimated coefficients. \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_all_dummy\n",
    "y = df_all_dummy.pop(\"BAD\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "print('Estimated logistic regression coefficients:\\n', lr.coef_)\n",
    "print(f'Model score is {lr.score(X, y):.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_nb_data_preparation (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
