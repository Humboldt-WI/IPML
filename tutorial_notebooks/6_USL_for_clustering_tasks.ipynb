{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432e8f1b-e758-4f8f-8b95-bfbb90fdcda2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/IPML/blob/master/tutorial_notebooks/6_USL_for_clustering_tasks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1207576-2ee0-4ef7-a0f7-cc333967feba",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Using Clustering\n",
    "<hr>\n",
    "\n",
    "This notebook revisits the lecture on **Unsupervised Machine Learning using Clustering** . There, we studied business applications of cluster analysis and familiarized ourself with the famous kMeans algorithm. This notebook comprises a set of demos and programming exercises to illustrate the corresponding concepts and exemplify relevant procedures.\n",
    "\n",
    "Key topics:\n",
    "- Synthetic data generation\n",
    "    - The `make_blobs`function \n",
    "    - The multivariate normal distribution\n",
    "- Visualizing 2D data using scatterplots\n",
    "- The kMeans algorithm\n",
    "    - Executing kMeans using `sklearn`\n",
    "    - The elbow method for determining the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64de401-3ee2-432f-b8d2-1bfb6e2cc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2717d-7000-49ab-b25f-21421089d102",
   "metadata": {},
   "source": [
    "# Creating synthetic data for clustering\n",
    "\n",
    "## Sampling from Gaussian distribution \n",
    "We use the function `make_blobs()` from the `sklearn.datasets` module to create synthetic data in 2D for clustering. You can set the number of clusters via the argument `centers`. Also, you can configure the function such that it returns the locations of the cluster centers. This will be useful for later comparisons. Thus, make use of this feature by setting `return_centers=True`.\n",
    "\n",
    "### Exercise 1: \n",
    "- Import the `make_blobs()` function\n",
    "- Familiarize yourself with its documentation\n",
    "- Use the function to generate 50 data points and store the results in a suitable data type\n",
    "- Visualize your synthetic data by means of a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4e83c-d69a-4980-b6e9-0d0ee74fb3b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### The multivariate normal distribution\n",
    "For better understanding of the `make_blobs()` function, the following code creates a 2D surface plot of the bi-variate normal distribution. First, we define a multivariate normal distribution with a mean of `mu=[0.0, 0.0]` and a covariance matrix \n",
    "```Python\n",
    "sigma = [[1.0, 0.0],[0.0, 1.0]]\n",
    "```\n",
    "Next, we create a grid of points at which we evaluate the density using the function `meshgrid()`. Last, we visualize the results using a *surface plot*. \n",
    "\n",
    " You can adjust the mean and covariance matrix to change the shape of the distribution. The `linspace` function is used to create the grid of points, and the `dstack` function is used to combine the x and y coordinates into a single array. The `multivariate_normal` function is used to create the distribution, and the `pdf` method is used to evaluate the distribution at the grid points. The `plot_surface` function is used to create the surface plot. \n",
    " \n",
    " Please make sure you have the `scipy` library installed in your Python environment. If needed, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ec75f-a44e-4ddd-adb2-5c0feb0229b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Define the mean and covariance matrix for the distribution\n",
    "mu = np.array([0.0, 0.0])\n",
    "cov = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "\n",
    "# Create a grid of points at which to evaluate the distribution\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = np.linspace(-3, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "pos = np.dstack((X1, X2))\n",
    "\n",
    "# Create the multivariate normal distribution and evaluate it at the grid points\n",
    "rv = multivariate_normal(mu, cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# Create the surface plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X1, X2, Z, cmap='viridis') \n",
    "ax.set_xlabel('$X_1$')\n",
    "ax.set_ylabel('$X_2$')\n",
    "ax.set_zlabel('Probability density', labelpad=1)\n",
    "plt.tight_layout()\n",
    "plt.title('Bi-Variate Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622a4ef",
   "metadata": {},
   "source": [
    "Intuitively, when using the `make_blobs()` function, the `centers` argument corresponds to the mean of the multivariate normal distribution, and the `cluster_std` argument corresponds to the standard deviation of the distribution. Hence, each cluster is associated with one multivariate (i.e., bi-variate in our case) normal distribution. For each cluster, the function samples the points from the corresponding multivariate normal distribution with the specified mean and standard deviation. \n",
    "\n",
    "## Exercise 2:\n",
    "Redo the previous exercise, but this time, create a synthetic data set with only two clusters. To that end:\n",
    "- Introduce a variable `mu` to store the cluster centers. Set the centers to be `(2, 2)` and `(4, 4)`.\n",
    "- Introduce a variable `std` (standard deviation) and set it to 0.5.\n",
    "- Generate 100 data points with the specified cluster centers and standard deviation using `make_blobs()`.\n",
    "    - Store the data points in a variable `X`.\n",
    "    - Store the true cluster index of each data point in a variable `y`.\n",
    "    - Store the cluster centers in a variable `centers`.\n",
    "- Visualize the synthetic data points and the cluster centers by reusing your solution to exercise 1.\n",
    "- Make sure that data points from different clusters receive different colors. To achieve this, you can set the argument `c` of the `scatterplot` function to the cluster index, which `make_blobs()` returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca681aa-c879-4c31-ab9f-c43df8a3f06c",
   "metadata": {},
   "source": [
    "# kMeans Clustering \n",
    "The following code, which is copied from the [sklearn documentation of `KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans), exemplifies how you can perform kMeans-based clustering. It illustrates the use of the `fit()` functions to execute the algorithm, how you can query the cluster to which the data points were assigned using the `.labels_` property, how you can cluster novel data points using the `predict()` function, and how you can access the found cluster centers using the `cluster_centers_` attribute, as well as the value of the kMeans objective, which is accessible via the `ìnertia_` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb90f9-76e0-4000-9800-04a6ce893e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create artificial data\n",
    "demo = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]) \n",
    "plt.scatter(demo[:, 0], demo[:, 1])\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Demo Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering of the demo data\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(demo)  # run the cluster algorithm\n",
    "print(kmeans.labels_)  # print the assignment of data points to clusters\n",
    "print(kmeans.predict([[0, 0], [12, 3]]))  # find the cluster of a novel data point\n",
    "print(kmeans.cluster_centers_)  # print the location of the cluster centroids\n",
    "print(kmeans.inertia_)  # print value of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad73e3e",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "- Drawing on the kMeans demo, run kMeans on your synthetic data created in exercise 2. \n",
    "- Set the number of clusters to *a suitable value*.\n",
    "- Print the cluster centers and the value of the kMeans objective.\n",
    "- Also print out the true cluster centers and compare them to those found by kMeans. What is your conclusions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24496202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e499a",
   "metadata": {},
   "source": [
    "Provided you solved exercise 3, you can run the following codes to visualize the results of the clustering. More specifically, the code visualizes the synthetic data points, whereby the color of a data point indicates its true true cluster membership and the symbol indicates to which cluster it has been assigned by kMeans. If a data point was assigned the *wrong* cluster, the code will highlight this data point with a thick, red border. Further, the code visualizes the true cluster centers (red crosses) and the cluster centers found by kMeans (yellow diamonds).\n",
    "\n",
    "For the code to work properly, you need to:\n",
    "- have a trained kMeans model stored in the variable `kmeans`\n",
    "- have the true cluster centers stored in the variable `centers`\n",
    "- have the synthetic data points stored in the variable `X`\n",
    "- have the true cluster index of each data point stored in the variable `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db63bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Create a scatter plot to visualize the clustering results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Synchronize cluster indices between y and kmeans.labels_\n",
    "\n",
    "# Compute the cost matrix\n",
    "cost_matrix = pairwise_distances(centers, kmeans.cluster_centers_)\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# Create a mapping from true cluster indices to kmeans cluster indices\n",
    "cluster_mapping = {row: col for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "# Map the true cluster indices to kmeans cluster indices\n",
    "mapped_labels = np.array([cluster_mapping[label] for label in y])\n",
    "\n",
    "# Plot the true clusters with different colors\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=mapped_labels, palette='viridis', style=y, edgecolor='k', s=100)\n",
    "\n",
    "# Highlight the misclassified points with red border color\n",
    "misclassified = (mapped_labels != kmeans.labels_)\n",
    "sns.scatterplot(x=X[misclassified, 0], y=X[misclassified, 1], facecolor='none', edgecolor='red', s=100, linewidth=1.5)\n",
    "\n",
    "# Plot cluster centers and centroids\n",
    "\n",
    "sns.scatterplot(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], c='yellow', marker='d', edgecolor='k', s=100)  # kMeans centroids\n",
    "sns.scatterplot(x=centers[:, 0], y=centers[:, 1], c='red', marker='x', s=200)  # Plot the true centers\n",
    "\n",
    "\n",
    "plt.title('kMeans Clustering Results')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(title='True Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af908b5",
   "metadata": {},
   "source": [
    "\n",
    ">**Remark:** The slightly complicated part of the above code concerns the highlighting of data points that kMeans misclassifies. The reason is that the index of a data points's true cluster is not necessarily the same as the index of the cluster to which kMeans assigns the data point. Consequently, a simple comparison `kmeans.labels_ == y` is insufficient to identify misclassified data points. Below, we address this issue by synchronizing the indices before highlighting misclassified data points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452a8d9-d688-4ba5-a955-d65449d0d802",
   "metadata": {},
   "source": [
    "## Elbow method to determine K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d09a48-395c-4136-abba-7b65b5dc91a1",
   "metadata": {},
   "source": [
    "The lecture introduced you to the elbow method. Recall that the elbow method is a heuristic to determine the number of clusters, which, in reality, we would not know.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/kmeans_elbow_method.png\" width=\"1280\" height=\"720\" alt=\"kMeans Algorithm\">\n",
    "\n",
    "To apply this method, we can make use of the `.inertia_` attribute of a fitted `KMeans` object. \n",
    "\n",
    "### Exercise 4\n",
    "- Create a list that stores candidate values for the meta-parameter *k* (i.e., number of clusters)\n",
    "    - Considers settings $k=1, 2, ..., 10$\n",
    "- Iterate over your list and run *kMeans* for each candidate setting of *k*. \n",
    "- Store, from each run of *kMeans, the value of the objective function, that is the property `kmeans.inertia_`\n",
    "- Create a line plot of *kMeans*´objective values against the corresponding number of cluster (i.e., associated candidate settings of *k*)\n",
    "\n",
    "Once you created a similar plot for the synthetic data, go back to the `make_blobs()` method and adjust its arguments to examine different types of data and their clustering. For example, examine how the elbow plot changes if you create data with three cluster centers, and examine how it changes if you create overlapping clusters. To achieve this, you can use the argument `cluster_std`, which the function provides.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 4\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
