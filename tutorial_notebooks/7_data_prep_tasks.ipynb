{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iGz0qnmS-UK"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/ipml/blob/master/tutorial_notebooks/8_data_prep_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHScrtl_S-UK"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "This notebook revisits the data preparation lecture. We investigate options to scale and categorize continuous features and properly encode categorical features before using them in a machine learning model. Much of the needed functionality is readily available in `Pandas`. For certain tasks, we will use transformation classess available in `sklearn.preprocessing`.\n",
    "\n",
    "For the notebook, we will make use of a novel dataset related to credit risk modeling. We begin with loading some standard packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33ZdGrYCS-UK"
   },
   "outputs": [],
   "source": [
    "# Load standard libraries\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HMEQ data set\n",
    "Our data set, called the  \"Home Equity\" or, in brief, HMEQ, is available via the [CreditRiskAnalytics website ](https://www.creditriskanalytics.net). It comprises  information about a set of borrowers, which are categorized along demographic variables and variables concerning their business relationship with the lender. A binary target variable called *BAD* is  provided and indicates whether a borrower has repaid their debt. You can think of the data as a standard use case of binary classification.\n",
    "\n",
    "The data set consists of 5,960 observations and 13 features including the target variable. The [website](https://www.creditriskanalytics.net) defines the variables as follows:\n",
    "\n",
    "- BAD: the target variable, 1=default; 0=non-default \n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim at describing the financial situation of a borrower. We will keep using the data set in future tutorials. Therefore, it makes sense to familiarize yourself with the above features.\n",
    "\n",
    "## Loading the data\n",
    "Let's start by loading the data and taking a look at the some entries. For simplicity, we provide a version in our [GitHub repository](https://github.com/Humboldt-WI/IPML/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HMEQ Data from our GitHub\n",
    "url = 'https://raw.githubusercontent.com/Humboldt-WI/IPML/master/data/hmeq.csv'\n",
    "hmeq = pd.read_csv(url)\n",
    "# Preview some entries\n",
    "hmeq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "hmeq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZL4c5HyS-UU"
   },
   "source": [
    "## Altering Data Types\n",
    "We start with a rather technical bit, data types. Recall the way our data is stored at the moment. \n",
    "The features *JOB* and *REASON* are stored as data type `object`. This is the most general data type in Python. A variable of this type can store pretty much any piece of data, numbers, text, dates, times, ... This generality has a price. Storing data as using type `object` consumes a lot of memory. Also, we cannot access specific functionality that is available for a specific data type only. Functions to manipulate text are an example. These are available for data of type `string` but not for data of type `object`. \n",
    "<br>\n",
    "In our case, the two features that Pandas stores as objects are actually categorical variables. We can easily verify this using, e.g., the `value_counts` method, which reports for every unique entry of a variable the number of occurrences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbBgoi3uS-UU",
    "outputId": "3fc66b34-74e2-472b-a185-0c929b7edc33"
   },
   "outputs": [],
   "source": [
    "print(hmeq.REASON.value_counts())  \n",
    "print('--------------')\n",
    "print(hmeq.JOB.value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4JsilV3S-UU"
   },
   "source": [
    "Based on the output, you can learn that *REASON* is a binary variable, whereas *JOB* is a categorical varaible with six different levels.\n",
    "\n",
    " Knowing the two features with type object are categories, we should alter their data type accordingly. To that end, we make use of the function `astype`, which facilitates converting one data type into another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MO6IxmTS-UU",
    "outputId": "8f52d60c-034d-4ecb-9fad-873995bbc710"
   },
   "outputs": [],
   "source": [
    "# Code categories properly \n",
    "hmeq['REASON'] = hmeq['REASON'].astype('category')\n",
    "hmeq['JOB'] = hmeq['JOB'].astype('category')\n",
    "hmeq.info()  # verify the conversion was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXA9BTp_S-UV"
   },
   "source": [
    "Although it does not really matter for this tiny data set, note that the conversion from object to category has reduced the amount of memory that the data frame consumes. On my machine, we need 524.2 KB after the translation, whereas we needed more than 600 KB for the original data frame. If you work with millions of observations the above conversion can result in a significant reduction of memory consumption. If memory consumption is an issue, we can achieve a significant further reduction by reducing the precision of the numerical variables. *Downcasting* from float64 to float32 is likely ok for predictive modeling. Also, the target variable is stored as an integer but we know that it has only two states, which indicate proper repayment of the loan or default. Thus, we can convert the target to a boolean. We perform these transformations in the next code demo. It does not introduce new Python concepts and is meant to simplify working with the data in the reminder. However, it is good practice to spend a little time on the codes and make sure you understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K5ln75FS-UV"
   },
   "outputs": [],
   "source": [
    "# The target variable has only two states so that we can store it as a boolean\n",
    "hmeq['BAD'] = hmeq['BAD'].astype('bool')\n",
    "\n",
    "# For simplicity, we also convert LOAN to a float so that all numeric variables are of type float\n",
    "hmeq['LOAN'] = hmeq['LOAN'].astype(np.float64)\n",
    "\n",
    "# Last, let's change all numeric variables from float64 to float32 to reduce memory consumption\n",
    "num_vars = hmeq.select_dtypes(include=np.float64).columns\n",
    "hmeq[num_vars] = hmeq[num_vars].astype(np.float32)\n",
    "\n",
    "# Finally, let's verify the data types\n",
    "hmeq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8t60vNRS-UV"
   },
   "source": [
    "In total, our type conversions reduced memory consumption by more than a half, from an initial value of >600KB to 250.7 KB. Bear this potential in mind when using your computer to process larger datasets. Should you be interested in some more information on memory efficiency, have a look at this post at [TowardDataScience.com](https://towardsdatascience.com/pandas-save-memory-with-these-simple-tricks-943841f8c32). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ypE7GuOS-UW"
   },
   "source": [
    "# Missing values\n",
    "Our data contains many missing values. This is easily seen when calling, e.g., `hmeq.head(5)`, and is common when working with real data. Likewise, handling missing values is a standard task in data preparation. `Pandas` provides the function `.isna()` as entry point to the corresponding functionality and helps with identifying the relevant cases.\n",
    "\n",
    "*Note*: `Pandas` also supports an equivalent function called `.isnull()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeahGMfES-UW",
    "outputId": "9141fa4f-01b3-4f3f-fa63-5e744bcb1c5f"
   },
   "outputs": [],
   "source": [
    "# Boolean mask of same size as the data frame to access missing values via indexing\n",
    "missing_mask = hmeq.isna()\n",
    "\n",
    "print(f'Dimension of the mask: {missing_mask.shape}')\n",
    "print(f'Dimension of the data frame: {hmeq.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the missing value mask\n",
    "missing_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aBde7TBS-UW"
   },
   "source": [
    "We can count the number of missing values per row or per column or in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_DgQ7QHS-UW",
    "outputId": "b931d0ae-fece-4d25-d8d9-0a352b4da582"
   },
   "outputs": [],
   "source": [
    "# missing values per row\n",
    "miss_per_row = missing_mask.sum(axis=1)\n",
    "print('Missing values per row:\\n', miss_per_row)\n",
    "print('-' * 50)\n",
    "# missing values per column\n",
    "miss_per_col = missing_mask.sum(axis=0)\n",
    "print('Missing values per column:\\n', miss_per_col )\n",
    "print('-' * 50)\n",
    "# count the total number of missing values\n",
    "n_total_missing = missing_mask.sum().sum()\n",
    "print(f'Total number of missing values: {n_total_missing}')\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_9jyQY_S-UW"
   },
   "source": [
    "It can be useful to visualize the *missingness* in a dataset using a heatmap, as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggtNOCJKS-UW",
    "outputId": "c66c163d-4ae4-46ec-978a-49cca891d75d"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(hmeq.isna(), cmap='viridis')  # quick visualization of the missing values in our data set\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the heatmap gives you a good intuition of how and where the dataset is affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnw8ySRgS-UW"
   },
   "source": [
    "## Categorical features\n",
    "Let's start with the two categorical features, `REASON` and `JOB`. We will treat them differently for the sake of illustration. Now that we start altering our data frame more seriously, it is a good idea to make a copy of the data so that we can easily go back to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuSqLNgzS-UW"
   },
   "outputs": [],
   "source": [
    "# copy data: we continue with altering the variable df while we keep variable hmeq for the raw data\n",
    "df = hmeq.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7nqgRZuS-UX"
   },
   "source": [
    "### Adding a new category level\n",
    "One way to treat missing values in a categorical feature is to introduce a new category level *IsMissing*. We will demonstrate this approach for the feature *REASON*. \n",
    "<br>One feature of the category data type in Pandas is that category levels are managed. We cannot add levels directly. Thus, before assigning the missing values our new category level *IsMissing*, we first need to introduce this level. We basically tell our data frame that *IsMissing* is another suitable entry for *REASON* next to the levels that already exist in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdBWbWtfS-UX",
    "outputId": "b0338de2-ff64-4ff7-d922-1142f7f92879"
   },
   "outputs": [],
   "source": [
    "# Variable REASON: we treat missing values as a new category level.\n",
    "# First we need to add a new level\n",
    "df.REASON = df.REASON.cat.add_categories(['IsMissing'])\n",
    "\n",
    "# Now we can do the replacement\n",
    "df.REASON[df.REASON.isna() ] = \"IsMissing\"\n",
    "df.REASON.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXAjFQFtS-UX",
    "outputId": "b5d3bf61-c04e-4d4f-a5f0-d345f8d3b184"
   },
   "outputs": [],
   "source": [
    "df.REASON.isna().sum()  # verify that no more missing values exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf_LOVmPS-UX"
   },
   "source": [
    "### Mode replacement\n",
    "For the feature *JOB*, which is multinomial, we replace missing values with the mode of the feature. Please note that this is a crude way to handle missing values. I'm not endorsing it! But you should have at least seen a demo. Here it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNUN63yS-UX",
    "outputId": "a5ecc720-55f7-48b2-967a-27b387f50ad1"
   },
   "outputs": [],
   "source": [
    "# Verify the counts for different levels of the feature JOB manually\n",
    "print(df.JOB.value_counts())\n",
    "\n",
    "# Determine the mode\n",
    "mode_of_job = df.JOB.mode()  # note that mode() returns a Series. \n",
    "mode_of_job = mode_of_job.iloc[0]  # Hence, to only get the mode value, we need to access the first element of the Series\n",
    "\n",
    "print('-' * 50)\n",
    "print(f'The mode of feature JOB is {mode_of_job}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual replacement, we use the `fillna` method of Pandas. It allows us to specify a value that is used to replace all missing values in a feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M32XJxQS-UY",
    "outputId": "a6d86590-c2d8-4716-adcf-9fa8e258b3ac"
   },
   "outputs": [],
   "source": [
    "# replace missing values with the mode\n",
    "df.JOB = df.JOB.fillna(mode_of_job)\n",
    "print(f'Number of remaining missing values in feature JOB: {df.JOB.isna().sum()}')  # verify that no more missing values exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZaNFpGS-UZ"
   },
   "source": [
    "## Numerical features\n",
    "We have a lot of numerical features. To keep things simple, we simply replace all missing values with the median. Again, this is  a crude approach that should be applied with care; if at all. However, it nicely shows how we can process several columns at once.\n",
    "\n",
    "Further, we discuss in the lecture that it can be useful to indicate the presence of a missing value in a numerical feature by adding a corresponding dummy variable. Our code illustrates that approach as well. Whenever we perform median replacement for a numerical feature, we add a new binary feature that indicates whether the original value was missing or not.\n",
    "\n",
    "\n",
    "<p align=\"left\" class=\"alert\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/missing_value_dummy.png\" alt=\"Indicating missing value replacement using dummy variables\" width=\"320\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjlpGozNS-UZ"
   },
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='float32').columns:  # loop over all numeric columns\n",
    "    if df[col].isna().sum() > 0:                         # check if there are any missing values in the current feature\n",
    "        dummies_col_name = col + '_ismissing'            # name of the new dummy variable\n",
    "        df[dummies_col_name] = df[col].isna().astype('int8')  # create the dummy variable indicating missingness\n",
    "        m = df[col].median(skipna=True)                  # compute the median of that feature\n",
    "        df[col].fillna(m, inplace=True)                  # replace missing values with the median\n",
    "        print(f'Processed feature {col}: added dummy variable {dummies_col_name} and replaced missing values with median {m}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukR_VzGS-UZ",
    "outputId": "13f49cee-b983-4e17-ddb3-ebef2477dff7"
   },
   "outputs": [],
   "source": [
    "# Verify there are no more missing values in the data\n",
    "n_total_missing = df.isna().sum().sum()\n",
    "if  n_total_missing == 0:\n",
    "    print('Well done, no more missing values!')\n",
    "else:\n",
    "    print(f'Ups! There are still {n_total_missing} missing values.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisit the data info. We expect to see several new columns for the introduced dummy variables\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## 1. Outliers\n",
    "The lecture introduced a rule of thumb saying that, for a given feature, a feature value $x$ can be considered an outlier if \n",
    "$$x >q_3(X) + 1.5 \\cdot IQR(X)$$\n",
    "\n",
    "where $q_3(X)$ denotes the third quantile of the distribution of feature $X$ and $IQR(X)$ the corresponding inter-quartile range. In the same scope, we also discussed the idea of *capping* outliers, i.e., replacing outlier values with the threshold value given by the above equation.\n",
    "\n",
    "<p align=\"left\" class=\"alert\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/outlier_treatment.png\" alt=\"Indicating missing value replacement using dummy variables\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "Here are your tasks:\n",
    "\n",
    "1. Use the `Pandas` method `quantile` to compute the third and first quartile of feature `LOAN`.\n",
    "2. Compute the threshold value that a feature value $x$ must not exceed according to the above equation. Store the result in a variable. \n",
    "3. Use logical indexing to identify all upper outliers in the feature `LOAN`.\n",
    "4. Create a new data frame that has no outliers in the feature `LOAN`. To that end: \n",
    "- Reuse your solution to task 3 to identify outliers using indexing\n",
    "- Change the `LOAN` values for all outlier cases to the threshold you computed in step 2.\n",
    "\n",
    "Follow-up, more advanced tasks:<br>\n",
    "\n",
    "5. Write a custom function that implements the functionality you created in task 4. Make the feature to work on an argument of your function.\n",
    "6. Call your custom function for all numerical features in the data frame. The goal is to create a data frame that does not have any upper outlier in any of its numerical features. To demonstrate the capabilities of your function, set the threshold to $3 \\cdot IQR(X)$. This way, only extreme outliers will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling numerical features\n",
    "Another common data preparation task is scaling numerical features. The goal is to ensure that all features have the same scale. This is important for many machine learning algorithms. The lecture introduced two common scaling methods: min-max scaling and z-score scaling.\n",
    "\n",
    "\n",
    "<p align=\"left\" class=\"alert\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/scaling_of_num_features.png\" alt=\"Scaling numerical features\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "The `sklearn` library provides implementations of both approaches in the classes `MinMaxScaler` and `StandardScaler`, which are part of the module `preprocessing`. Experiment with these classes to solve the following exercises.\n",
    "\n",
    "1. Import the class `MinMaxScaler` and `StandardScaler` from the module `preprocessing` in the library `sklearn`.\n",
    "2. Familiarize yourself with the functioning of the `StandardScaler` using its documentation and other sources (e.g., web search). \n",
    "3. Test the `StandardScaler` by applying it to the numerical features `LOAN`. Afterwards, the scaled feature should have a mean of 0 and a standard deviation of 1. Write a few lines of code to verify this.\n",
    "4. The use of the `MinMaxScaler` is similar to the `StandardScaler`. Apply the `MinMaxScaler` to all other numerical features in the data set. More specifically, \n",
    "- Create a new data frame that contains only the numerical features.\n",
    "- Remove the feature `LOAN` from that data frame; as we already scaled it in task 3.\n",
    "- Apply the `MinMaxScaler` to the new data frame.\n",
    "- Write a few lines of code to verify that the scaling was successful. To that end, recall what the 'MinMaxScaler' does.\n",
    "- Combine the scaled features with the feature `LOAN` and the categorical features in a new `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discretizing numerical features\n",
    "Discretizing numerical features is another common data preparation task. The goal is to convert continuous numerical features into discrete bins or categories. This can be useful for certain types of analysis and modeling. \n",
    "\n",
    "<p align=\"left\" class=\"alert\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/binning_num_features.png\" alt=\"Binning of numerical features\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "The `pandas` library provides the `cut` and `qcut` functions for this purpose.\n",
    "\n",
    "1. Familiarize yourself with the `cut` and `qcut` functions in the `pandas` library using their documentation and other sources (e.g., web search).\n",
    "2. Use the `cut` function to discretize the `LOAN` feature into 5 equal-width bins. Assign meaningful labels to each bin (e.g., 'Very Low', 'Low', 'Medium', 'High', 'Very High').\n",
    "3. Verify the binning by displaying the first few rows of the data frame and checking the `LOAN` feature.\n",
    "4. Use the `qcut` function to discretize the `MORTDUE` feature into 4 quantile-based bins. Assign meaningful labels to each bin (e.g., 'Q1', 'Q2', 'Q3', 'Q4').\n",
    "5. Verify the binning by displaying the first few rows of the data frame and checking the `MORTDUE` feature.\n",
    "\n",
    "Follow-up, more advanced tasks:<br>\n",
    "\n",
    "6. Create a new data frame that includes the discretized `LOAN` and `MORTDUE` features along with the other original features.\n",
    "7. Write a custom function that takes a data frame and a list of numerical features as input and returns a new data frame with all specified features discretized into a given number of bins using the `cut` function. Test your function on the numerical features in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to Exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding categorical features\n",
    "Encoding categorical features is another important data preparation task. Machine learning algorithms require numerical input, so we need to convert categorical features into numerical format. The `pandas` library provides the `get_dummies` function for this purpose. This function will turn a categorical feature with k levels into k binary features, as shown in this example for a categorical feature COLOR:\n",
    "\n",
    "**Original Table**\n",
    "\n",
    "| ID | COLOR  |\n",
    "|----|--------|\n",
    "| 1  | Red    |\n",
    "| 2  | Blue   |\n",
    "| 3  | Green  |\n",
    "| 4  | Red    |\n",
    "| 5  | Green  |\n",
    "\n",
    "**Table After Dummy Coding**\n",
    "\n",
    "| ID | COLOR_Red | COLOR_Blue | COLOR_Green |\n",
    "|----|-----------|------------|-------------|\n",
    "| 1  | 1         | 0          | 0           |\n",
    "| 2  | 0         | 1          | 0           |\n",
    "| 3  | 0         | 0          | 1           |\n",
    "| 4  | 1         | 0          | 0           |\n",
    "| 5  | 0         | 0          | 1           |\n",
    "\n",
    "Let's move on to some exercises.\n",
    "\n",
    "\n",
    "1. Familiarize yourself with the `get_dummies` function in the `pandas` library using its documentation and other sources (e.g., web search).\n",
    "2. Apply the `get_dummies` function to discretize the `REASON` feature. Assign meaningful column names to the resulting dummy variables.\n",
    "3. Verify the result of the previous dummy-coding step. You should see that the `get_dummies` function has created three binary variables, one for each of the three levels of the *REASON* feature. Recalling the functioning of regression methods, you now face a problem. The three new dummy variables are **linearly dependent**. Regression models will not work with linearly dependent features. Redo task 3 but this time pay attention to the argument `drop_first`, which the `get_dummies` function offers. Set this argument in such a way that you avoid linear dependency among the created dummy variables. \n",
    "\n",
    "Additional, more advanced exercises:<br>\n",
    "\n",
    "4. This task assumes you solved exercise 7 from the above tasks concerning discretization. If you have not done so, go back to the discretization exercises and solve all tasks including task 7.\n",
    "5. Discretize all numerical features in the data frame using the custom function you created in task 7. Apply the `get_dummies` function to the discretized features in the resulting data frame while avoiding linear dependency. Next, create a data frame `df_all_dummy` that includes only the dummy-encoded features. \n",
    "6. Fit a logistic regression model to the `df_all_dummy` data frame. Use the class `LogisticRegression` from the module `sklearn.linear_model`. The handling of `LogisticRegression` is similar to the handling of `LinearRegression`, which we examined in [Tutorial 5](https://github.com/Humboldt-WI/IPML/blob/main/tutorial_notebooks/5_SML_for_regression_solutions.ipynb). Train the model using the `fit()` method. Then, print the estimated coefficients and compute the model's performance on the training set using the method `score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to Exercise 4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_nb_data_preparation (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
