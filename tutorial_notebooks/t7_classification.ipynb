{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26db1d15-3d4d-4cb4-bb99-f1d9492aab18",
   "metadata": {
    "id": "84bf70f4-915d-4738-b78c-700b27b06f38"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/IPML/blob/master/tutorial_notebooks/t7_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bfeb4e-f90e-4231-96c5-65c71b737bda",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# Classification Models: The case of credit default prediction\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c084741-bb8f-48c2-afbe-5236af75860b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1695536714957,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "wIgF2_GabxOZ"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a877c9-43f5-44bb-bc02-e6d171f5f728",
   "metadata": {
    "id": "hk3ijnvKemS5"
   },
   "source": [
    "## Data preparation\n",
    "In this tutorial, we revisit the logistic regression model and study how it allows us to approach binary classification problems. To that end, we consider the case of credit default prediction considering the *Home Equity (HMEQ)* data set from the famous [Credit Risk Analytics](http://www.creditriskanalytics.net) textbook. It comprises information about a set of borrowers, which are categorized along demographic features and features concerning their business relationship with the lender. A binary target variable called 'BAD' is provided and indicates whether a borrower has repaid their debt. Here is an overview of the data:\n",
    "- BAD: the target variable, 1=default; 0=non-default\n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim at describing the financial situation of a borrower, which should probably tell us something about the risk of a borrower to default.\n",
    "\n",
    "Using the `Pandas`library, we can retrieve the data right from the web; specifically the GitHub repository of our course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edad3a7-a8e5-4e5b-9c25-50c288947915",
   "metadata": {
    "id": "28M_8V9LkG6Y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>25860.0</td>\n",
       "      <td>39025.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>70053.0</td>\n",
       "      <td>68400.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>16700.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1700</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Office</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BAD  LOAN  MORTDUE     VALUE   REASON     JOB   YOJ  DEROG  DELINQ  \\\n",
       "0    1  1100  25860.0   39025.0  HomeImp   Other  10.5    0.0     0.0   \n",
       "1    1  1300  70053.0   68400.0  HomeImp   Other   7.0    0.0     2.0   \n",
       "2    1  1500  13500.0   16700.0  HomeImp   Other   4.0    0.0     0.0   \n",
       "3    1  1500      NaN       NaN      NaN     NaN   NaN    NaN     NaN   \n",
       "4    0  1700  97800.0  112000.0  HomeImp  Office   3.0    0.0     0.0   \n",
       "\n",
       "        CLAGE  NINQ  CLNO  DEBTINC  \n",
       "0   94.366667   1.0   9.0      NaN  \n",
       "1  121.833333   0.0  14.0      NaN  \n",
       "2  149.466667   1.0  10.0      NaN  \n",
       "3         NaN   NaN   NaN      NaN  \n",
       "4   93.333333   0.0  14.0      NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "df = pd.read_csv(url)  # standard pandas function to load tabular data in CSV format\n",
    "df.head(5)  # obtain a preview of the data by showing the first 5 rows of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711aeb5-c3e2-4962-a9aa-9a2ae399e5ee",
   "metadata": {
    "id": "DcovK44VAdAR"
   },
   "source": [
    "### Missing Values\n",
    "As shown above, the data exhibits missing values. We have discussed missing values in the scope of our [EDA lecture](https://moodle.hu-berlin.de/pluginfile.php/5910736/mod_resource/content/2/ipml_s3_eda.pdf). This is not the time to revisit that material. To use the data set, we simply replace missing values using the median and mode of numerical and categorical features respectively.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9a3646-2afb-453d-8126-c885b23fd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When starting, the data exhibits 5271 missing values\n",
      "At this point, the data exhibits 0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Missing value handling\n",
    "print('When starting, the data exhibits {} missing values'.format(df.isna().sum().sum()))\n",
    "\n",
    "for col in df.columns:  # loop through all the columns (i.e., features)\n",
    "    if df[col].dtype == 'O':  # decide on the imputation strategy based on the data type\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)  # model replacement for categories\n",
    "    else:\n",
    "        df[col].fillna(df[col].median(), inplace=True)  # Mean replacement for all other features \n",
    "\n",
    "# Verify that the data frame does not exhibit missing values anymore\n",
    "print('At this point, the data exhibits {} missing values'.format(df.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da73fda-3037-41a8-a4f0-e6d1d2542587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>25860.0</td>\n",
       "      <td>39025.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>34.818262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>70053.0</td>\n",
       "      <td>68400.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.818262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>16700.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.818262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>65019.0</td>\n",
       "      <td>89235.5</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>Other</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.818262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1700</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Office</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.818262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BAD  LOAN  MORTDUE     VALUE   REASON     JOB   YOJ  DEROG  DELINQ  \\\n",
       "0    1  1100  25860.0   39025.0  HomeImp   Other  10.5    0.0     0.0   \n",
       "1    1  1300  70053.0   68400.0  HomeImp   Other   7.0    0.0     2.0   \n",
       "2    1  1500  13500.0   16700.0  HomeImp   Other   4.0    0.0     0.0   \n",
       "3    1  1500  65019.0   89235.5  DebtCon   Other   7.0    0.0     0.0   \n",
       "4    0  1700  97800.0  112000.0  HomeImp  Office   3.0    0.0     0.0   \n",
       "\n",
       "        CLAGE  NINQ  CLNO    DEBTINC  \n",
       "0   94.366667   1.0   9.0  34.818262  \n",
       "1  121.833333   0.0  14.0  34.818262  \n",
       "2  149.466667   1.0  10.0  34.818262  \n",
       "3  173.466667   1.0  20.0  34.818262  \n",
       "4   93.333333   0.0  14.0  34.818262  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1706d-6894-4514-b1e4-178e72db55c5",
   "metadata": {},
   "source": [
    "### Category encoding\n",
    "The data set comprises two categorical variables. We must encode this before moving on. *Dummy coding* is a standard approach to do so. Dummy coding is best explained by examining how it chances our two categorical variables REASON and JOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1277a17-3646-4fa6-84cf-f4bfa3325719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5960 entries, 0 to 5959\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   BAD      5960 non-null   int64  \n",
      " 1   LOAN     5960 non-null   int64  \n",
      " 2   MORTDUE  5960 non-null   float64\n",
      " 3   VALUE    5960 non-null   float64\n",
      " 4   REASON   5960 non-null   object \n",
      " 5   JOB      5960 non-null   object \n",
      " 6   YOJ      5960 non-null   float64\n",
      " 7   DEROG    5960 non-null   float64\n",
      " 8   DELINQ   5960 non-null   float64\n",
      " 9   CLAGE    5960 non-null   float64\n",
      " 10  NINQ     5960 non-null   float64\n",
      " 11  CLNO     5960 non-null   float64\n",
      " 12  DEBTINC  5960 non-null   float64\n",
      "dtypes: float64(9), int64(2), object(2)\n",
      "memory usage: 605.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f5cf89-ffd3-446e-9136-f9a65232586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5960 entries, 0 to 5959\n",
      "Data columns (total 17 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   BAD             5960 non-null   int64  \n",
      " 1   LOAN            5960 non-null   int64  \n",
      " 2   MORTDUE         5960 non-null   float64\n",
      " 3   VALUE           5960 non-null   float64\n",
      " 4   YOJ             5960 non-null   float64\n",
      " 5   DEROG           5960 non-null   float64\n",
      " 6   DELINQ          5960 non-null   float64\n",
      " 7   CLAGE           5960 non-null   float64\n",
      " 8   NINQ            5960 non-null   float64\n",
      " 9   CLNO            5960 non-null   float64\n",
      " 10  DEBTINC         5960 non-null   float64\n",
      " 11  REASON_HomeImp  5960 non-null   bool   \n",
      " 12  JOB_Office      5960 non-null   bool   \n",
      " 13  JOB_Other       5960 non-null   bool   \n",
      " 14  JOB_ProfExe     5960 non-null   bool   \n",
      " 15  JOB_Sales       5960 non-null   bool   \n",
      " 16  JOB_Self        5960 non-null   bool   \n",
      "dtypes: bool(6), float64(9), int64(2)\n",
      "memory usage: 547.2 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(data=df, drop_first=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c144882-4eb8-472c-8160-13526ca8630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our standard variables X,y for subsequent tasks\n",
    "X = df.copy()\n",
    "y = X.pop('BAD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad375a-8cc8-40a8-be5d-12fc6682fa89",
   "metadata": {},
   "source": [
    "## Exercise 1: Plotting data for classification\n",
    "You will remember the many plots we came across when discussing regression. We also saw some analog plots for classification problems in the [lecture](https://moodle.hu-berlin.de/pluginfile.php/5941409/mod_resource/content/1/ipml_s6_classification.pdf). One of them was a 2d scatter plot displaying the bi-variate relationship between one feature and the binary target variable. \n",
    "<br>\n",
    "<img alt=\"Classification problem in 2D\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/1d_classification_problem.PNG\" width=600 height=800/>\n",
    "<br><br>\n",
    "Another was the 2d scatter plot showing two selected features and distinguishing examples of the two classes using colors.\n",
    "<br>\n",
    "<img alt=\"Classification problem in 2D\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/2d_classification_problem.png\" width=600 height=800/>\n",
    "<br><br>\n",
    "\n",
    "Your first task is to create a similar plots for the credit data. In principle, you can select any combination of features that you like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056e886-fb90-4846-a961-fe6543a6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94cc3f5-c8b6-486f-b88c-160071a3e13b",
   "metadata": {},
   "source": [
    "## Exercise 2: Model adequacy  \n",
    "We introduced logistic regression as an extension of linear regression for cases in which we work with a binary target variable. Nonetheless, just as linear regression, logistic regression is a (generalized) linear model. It assumes that feature values determine the target via a linear, additive functional relationship.\n",
    "\n",
    "Play with the above code to plot the distribution of different combinations of the features. Eventually, you should arrive at a preliminary conclusion of whether logistic regression is a suitable model for the credit risk data at hand. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2077f259-bce7-4d0b-acc0-7ca56c9a9d30",
   "metadata": {},
   "source": [
    "**Do you think logistic regression is suitable for the data? Briefly explain your reasoning.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Also, has the analysis of different scatter plots revealed strong features that will facilitate predicting class membership (i.e., repayment behavior)?**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f72cb-ad10-4651-be21-0c50e4f3a1ee",
   "metadata": {},
   "source": [
    "## Exercise 3: Logistic regression\n",
    "Time to estimate our first model. Unsurprisingly, our goto library `sklearn` supports logistic regression through the class `LogisticRegression`, which is available in the module `sklearn.linear_models`. \n",
    "\n",
    "A nice feature of `sklearn`is that it facilitates using different ML algorithms through a common interface. This means that the way in which you use, for example train and test, a model is basically the same no matter which learning algorithm you use. Last week, we examined the implementation of linear regression. So, the way to use logistic regression should be very similar. Feel free to draw on the examples from last week and/or the corresponding [solution notebook](https://github.com/Humboldt-WI/IPML/blob/main/tutorial_notebooks/t6_regression_solution.ipynb). You task is to:\n",
    "- partition our data into a 75% training and 25% test set,\n",
    "- train a logistic regression model on the training set,\n",
    "- and compute predictions for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688ec3f-a432-42cc-900c-92948a10f638",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b595d6-a9dc-4acf-b44e-f28dab745736",
   "metadata": {},
   "source": [
    "Note that the `sklearn` implementation does not provide an informative summary, as did the library `statsmodels`, which you saw in our [regression notebook](https://github.com/Humboldt-WI/IPML/blob/main/tutorial_notebooks/t6_regression_solution.ipynb). Basically, the reason is that `sklearn` is designed to support prediction. Let's demonstrate how to do this, that is compute predictions using the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937846bf-2f3d-4d58-9dd4-02ce7747d319",
   "metadata": {},
   "source": [
    "### Test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856dfd-fe11-4ed1-803e-63f66c58b691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d77fa3e9-1b6e-4f07-8bae-3752b473fc88",
   "metadata": {},
   "source": [
    "## Exercise 4: Diagnosing predictions\n",
    "\n",
    "### 4a) Classification accuracy and the score function\n",
    "Likely, you are also interested in assessing the model. There is an easy way to do this. Just call the function `score()`, which the trained logistic regression model supports. More specifically, assume the variable that you created above is called `lr_model`, and assume further that your feature matrix and target variable are called `X` and `y`, respectively. Then, you can run:\n",
    "```\n",
    "lr_model.score(X,y)\n",
    "```\n",
    "Try it out but remember to adjust the names of the variables appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7d6de-713f-4060-bda9-080130bc9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the score of the logistic regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecb6e7-94f1-4a5c-8d41-697616b550ee",
   "metadata": {},
   "source": [
    "Supposedly, you will have observed a score of about 80 percent, which does sound pretty good, right? Well, to be sure we should better understand for start what this number means, that is, what *score* we have just calculated. It turns out that when you call the `score()` function for a classification model, you receive an estimate of that model's classification accuracy as result. Regression models would produce a different *score*. To verify your score is indeed the classification accuracy, recalculate classification *manually*. In case, recall how we defined classification accuracy in the lecture:\n",
    "\n",
    "<br>\n",
    "<img alt=\"Classification problem in 2D\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/confusion_matrix.PNG\" width=600 height=800/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca68134-248f-4ef5-a370-4fef10a52742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compute (test set) accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a468885-d4c4-420c-a214-f58c7d7a457f",
   "metadata": {},
   "source": [
    "### 4b) Comparing to a naive classifier\n",
    "Interpreting classification accuracy, and more generally the performance of any predictive model, it is useful to compare is to a baseline. But what baseline? We face a classification problem. There are two classes, good payers and bad payers, and we aim to tell these apart. Come up with a very basic - naive - strategy to solve the classification problem without using any model. Write a piece of code to calculate the classification accuracy of your naive strategy and compare to logistic regression. \n",
    "> Hint: if you feel a bit lost, consider web searching for *dummy classifier* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce23f45-5e1f-4cf8-b79c-68d19d86f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate the score of a naive classifier or dummy classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e749a3-67f7-4a8e-bcce-7e02d00347a4",
   "metadata": {},
   "source": [
    "### 4c) Confusion matrix\n",
    "The comparison of logistic regression to a naive classifier should have revealed that your model is not that strong after all. Classification accuracy of almost 80% sounds good at first glance but is far less impressive when knowing that naively predicting the majority class gives basically the same result. To get a better understanding of how well (or poorly) the model classifiers, your next task is to produce a confusion matrix. You can find all the functionality that you need in the class `ConfusionMatrixDisplay`, which is part of the `sklearn.metrics` module. So, import that class, examine how it works using the documentation, and plot a beautiful confusion matrix for your logistic regression model. Once you acomplished this task, make sure to study the result and discuss the quality of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433f558-cf56-476e-84d0-c44d89517d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db67deff-dce2-4443-a49f-c2000a166701",
   "metadata": {},
   "source": [
    "## Exercise 5: ROC Analysis\n",
    "Previous analysis suggests that the logistic regression model was not doing a good job. It barely recognizes the bad risks, which is the key objective of a credit default prediction model. That said, don't be too quick with dismissing the model. Recall that the confusion matrix is based on a cut-off value:\n",
    "\n",
    "<br>\n",
    "<img alt=\"Classification problem in 2D\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/classification_cutoff.PNG\" width=600 height=800/>\n",
    "<br><br>\n",
    "\n",
    "Up to this point, we did not set the cut-off value anywhere. Given this, it is actually surprising that we have been able to create a confusion matrix in the previous exercise. The reason for this is that `sklearn` assumes a default cut-off value of 0.5. This cut-off is used whenever you call the function `predict()` for a classifier. In our opinion, this makes the `predict()` function rather useless for classification. For classification problems, you should routinely compute probability predictions. If you seek discrete class predictions, compare the model-based probability predictions to a cut-off; as illustrated in the above picture. How to choose a **suitable** cut-off is a nontrivial question and out-of-the-scope of this notebook. Rather, as the title of the exercise has already indicated, we want to revisit ROC analysis. To that end, produce probability forecasts for the test data set using your logistic regression model.\n",
    "\n",
    "### 5a) Probability predictions\n",
    "Compute probabilistic predictions using the function `predict_proba()`. Create a variable `yhat_prob` to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e614b3b-72a4-4989-908a-5458b90ff85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a03f481-cb97-4dd4-aad2-6c3dfc951b48",
   "metadata": {},
   "source": [
    "Note how predict_proba returns the probabilities of both classes. This appears awkward. Given they add up to one, why have both? The answer is that `sklearn` aims to be consistent. Whenever you face a multi-class problem, it is essential to return the estimated probabilities of each class. Thus, for the sake of consistency, the implementation of the `predict_proba()` function is such that it returns probability estimates for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933784a-3d06-49db-b9a7-674776736e89",
   "metadata": {},
   "source": [
    "### 5b) ROC curve\n",
    "The probabilistic predictions are the foundation of the ROC curve. Roughly speaking, we compare them to all possible cut-off values, derive all possible confusion matrices, extract the true positive rate and false positive rates from these confusion matrixes, and plot those values against one another. Of course, we do not have to do this by hand. Instead, sklearn provides the corresponding functionality via several functions and classes including `roc_curve`, `RocCurveDisplay()`, and, if you only care about the AUC score, `roc_auc_score`. All of them are available in the module `sklearn.metrics`. Your task is to import these functions and familiarize yourself with the functionality. Begin with the class `RocCurveDisplay()`, which should quickly produce you a nice ROC curve. Afterward, try to augment your curve using the diagonal line. A discussed in the lecture, a diagonal line in ROC space represents the performance of guessing classes at random. Next, we create the plot using the function `roc_curve`, which offers more flexibility but also requires you to write more code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a7cb-f1d2-4981-b0e0-7c73eb457a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c853d8-09cc-4341-8506-58d06f51a191",
   "metadata": {},
   "source": [
    "### 5c) K-fold cross-validation\n",
    "Recall that $K$-fold Cross-Validation allows us to evaluate a model's performance and generalization capability. The process involves dividing the dataset into $K$ subsets or \"folds\". The model is then trained on $K-1$ folds and tested on the remaining one, repeating this process $K$ times. The resulting $K$ performance scores provide a robust assessment of the model's overall capability, making it an essential practice when developing machine learning models using Python and similar languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90aab4-2ff6-4e41-b23b-aa5c6acd2b42",
   "metadata": {},
   "source": [
    "<img alt=\"Cross-validation\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/cross_validation1.png\" width=600 height=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbfcb5-5d73-4c39-8201-510a1bcbcf8a",
   "metadata": {},
   "source": [
    "Your task is to apply $K$-fold cross-validation using the `KFold()` class from the module `sklearn.model_selection` to get a robust estimate of the AUC ROC. First import the `KFold()` class. Using its documentation, web search, and perhaps GenAI, write code to cross-validate the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a64dc-615d-4d92-8b0d-f5f0447578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation of the logistic regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704783bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91dee4c3-0306-4d51-881d-0df740886766",
   "metadata": {},
   "source": [
    "## The visual logistic regression\n",
    "We complete this part with a visual demo of the logistic regression model. No more work for you, just execute the code and enjoy. And of discuss the plot to make sure you understand how it depicts the classification solution of our logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a3c5d-614f-4ffb-9fd2-f67d66b0ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logit_decision_surface(model, y, data, x1, x2, save_fig=False):\n",
    "    '''\n",
    "        Visualization of logistic regression in 2D\n",
    "        \n",
    "        Creates a plot depicting the distribution of the input\n",
    "        data along two dimensions and the probability predictions\n",
    "        of a logistic regression model. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model :   An instance of the sklearn class LogisticRegression,  which        \n",
    "                  has been trained on the input data.\n",
    "\n",
    "        y     :   The true outcomes of the target variable \n",
    "        \n",
    "        data  :   Pandas data frame providing the feature values.\n",
    "\n",
    "        x1, x2:   The function plots the results of logistic regression in\n",
    "                  two dimensions. The parameters x1 and x2 give the names\n",
    "                  of the features used for plotting. These features will be\n",
    "                  extracted from the data frame.\n",
    "\n",
    "        save_fig: Binary variable allowing you to save the figure as a PNG image. \n",
    "                  Default: False\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        The function does not return a result. It's purpose is to visualize \n",
    "        logistic regression model. The corresponding plot is the only output.\n",
    "    '''\n",
    "\n",
    "    #if len(model.coef_.ravel())!=2:\n",
    "    #    raise Exception('Please estimate a logit model using only two features!')\n",
    "    # Define some variables to govern the plot\n",
    "    bounds = data.describe().loc[[\"min\", \"max\"]][[x1, x2]].to_numpy()  # value ranges of the two features\n",
    "    eps = 5  # tolerance parameter \n",
    "\n",
    "    # Create hypothetical data points spanning the entire range of feature values.\n",
    "    # We need these to get from our logistic regression model a probability prediction\n",
    "    # for every possible data point\n",
    "    xx, yy = np.mgrid[(bounds[0,0]-eps):(bounds[1,0]+eps), (bounds[0,1]-eps):(bounds[1,1]+eps)]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Perhaps the logistic regression model was fitted using the full data frame. \n",
    "    # To also work in that case, we extract the estimated regression coefficients \n",
    "    # corresponding to the two features we consider for plotting\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(model.feature_names_in_)}  # create a dic as intermediate step\n",
    "    indices = [feature_to_index[f] for f in [x1, x2]]  # Find the indices of our two features of interest using the dic\n",
    "    w = model.coef_.ravel()[indices]  # estimated regression coefficients\n",
    "    b = model.intercept_  # estimated intercept of the logistic regression model\n",
    "\n",
    "    # Compute probability predictions over the entire space of possible feature values\n",
    "    # In the interest of robustness, we manually compute the logistic regression predictions\n",
    "    # using the regression coefficients extracted above\n",
    "    probs = 1/(1+np.exp(-(np.dot(grid, w.reshape(2,-1))+b))).reshape(xx.shape)\n",
    "\n",
    "    # We are finally ready to create our visualization\n",
    "    f, ax = plt.subplots(figsize=(8, 6))  # new figure\n",
    "    # Contour plot of the probability predictions across the entire feature range\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)  \n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$\\hat{p}(y=1|X)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    # Scatter plot of the actual data\n",
    "    ax.scatter(data[x1], data[x2], c=y, s=50, cmap=\"RdBu\", vmin=0, vmax=1,\n",
    "               edgecolor=\"white\", linewidth=1);\n",
    "    plt.xlabel(x1)\n",
    "    plt.ylabel(x2)\n",
    "    if save_fig==True:\n",
    "        plt.savefig('logit_contour.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calling the function\n",
    "plot_logit_decision_surface(lr_model, ytr, Xtr, 'DEBTINC', 'YOJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62b102-376a-4607-bf7d-972677ac7629",
   "metadata": {},
   "source": [
    "# Well done! This was another comprehensive set of exercises."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
