{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/IPML/blob/master/tutorial_notebooks/9_classification_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Classification \n",
    "In this demo notebook, we will revisit our lecture on classification models. To that end, we consider the logistic regression model and study how it allows us to approach a probability-to-default prediction task. As usual, we provide ready-to-use demo codes and small programming tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We continue using the HMEQ classification data sets. Beyond loading standard libraries, the following code block reads the data from our [GitHub repository](https://github.com/Humboldt-WI/IPML/tree/main) and performs some preprocessing operations, which we introduced in earlier tutorials. Since future tutorials will need the same functionality, we encapsulate the code in a function called `get_credit_risk_data()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We put all codes to retrieve and prepare our credit risk data into a custom function\n",
    "def get_credit_risk_data(outlier_factor=2):\n",
    "    # Load credit risk data directly from GitHub\n",
    "    data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "    hmeq = pd.read_csv(data_url)\n",
    "\n",
    "    # Code categories properly \n",
    "    hmeq['REASON'] = hmeq['REASON'].astype('category')\n",
    "    hmeq['JOB'] = hmeq['JOB'].astype('category')\n",
    "    hmeq = pd.get_dummies(hmeq, drop_first=True)  \n",
    "    \n",
    "    # Code the target variable properly\n",
    "    hmeq['BAD'] = hmeq['BAD'].astype('bool')\n",
    "    \n",
    "    # Downcast numerical columns to save memory\n",
    "    ix_numeric_columns = hmeq.select_dtypes(include=np.number).columns\n",
    "    hmeq[ix_numeric_columns] = hmeq[ix_numeric_columns].astype('float32')\n",
    "\n",
    "    # Handle missing values:\n",
    "    # 1. The feature DEBTINC is important but suffers many missing values. Blindly replacing these missing values\n",
    "    #    would introduce bias and harm any model trained on the data. To avoid this, we add a dummy variable\n",
    "    #    to indicate whether the feature value was missing or not.\n",
    "    hmeq['D2I_miss'] = hmeq['DEBTINC'].isna().astype('category')\n",
    "    # 2. For the other numerical features, we use the median to impute missing values. For the categorical features\n",
    "    imputer = SimpleImputer(strategy='median')  # Create an imputer object with the strategy 'median'\n",
    "    hmeq[ix_numeric_columns] = imputer.fit_transform(hmeq[ix_numeric_columns])  \n",
    "    # 3. For the categorical features, we use the mode to impute missing values\n",
    "    ix_cat = hmeq.select_dtypes(include=['category']).columns  # Get an index of the categorical columns\n",
    "    for c in ix_cat:  # Process each category\n",
    "        hmeq.loc[hmeq[c].isna(), c ] = hmeq[c].mode()[0]  # the index [0] is necessary as the result of calling mode() is a Pandas Series\n",
    "    \n",
    "    # Truncate outliers among numerical features\n",
    "    if outlier_factor > 0:\n",
    "        for col in ix_numeric_columns:\n",
    "            if col not in ['DELINQ', 'DEROG']:  # We do not truncate these features as their distribution if strongly skewed such outlier trunction would leave us with a constant feature\n",
    "                q1 = hmeq[col].quantile(0.25)\n",
    "                q3 = hmeq[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - outlier_factor * iqr\n",
    "                upper_bound = q3 + outlier_factor * iqr\n",
    "                hmeq[col] = hmeq[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    hmeq[ix_numeric_columns] = scaler.fit_transform(hmeq[ix_numeric_columns])\n",
    "\n",
    "    # Separate the target variable and the feature matrix\n",
    "    y = hmeq.pop('BAD')\n",
    "    X = hmeq\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Call the function to retrieve the data\n",
    "X, y = get_credit_risk_data()   \n",
    "\n",
    "# Preview the data\n",
    "X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477e7a-c3c4-4952-b50b-4cd3d866906c",
   "metadata": {},
   "source": [
    "# Binary classification for PD modeling\n",
    "Having prepared our data, we can proceed with predictive modeling. The lecture introduced the general classification setup and the logistic regression model. Let's revisit these elements in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2ea9f-4363-42a7-a5a3-ec2d037cff01",
   "metadata": {},
   "source": [
    "## Excercise 1: Plotting data for classification\n",
    "You will remember the many plots we came across when discussing regression. We also saw some analog plots for classification problems in the lecture. One of them was a 2d scatter plot displaying the bi-variate relationship between selected features and the binary target variable. \n",
    "\n",
    "![Classification problem in 2D](https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/2d_classification_problem.png)\n",
    "\n",
    "Your first task is to create a similar plot for the credit data. In principle, you can select any combination of features that you like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056e886-fb90-4846-a961-fe6543a6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742e463-9ab9-49ca-a84e-3c1bfb004a90",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "Time to estimate our first classification model. We will use logistic regression. Think of it as an extension of linear regression for cases in which we work with a binary target variable. Just as in linear regression, logistic regression involves model training on labelled data. The below code uses the `sklearn` library to train a logistic regression-based classification model. In case you receive a warning message when running the code (i.e., *Convergence warning*), please ignore this message for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=888)  # we define a random_state to ensure that we get the same results when re-running this cell multiple times\n",
    "logreg.fit(X, y)  # Initiate training by calling the fit method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e698f",
   "metadata": {},
   "source": [
    "Likely, you are also interested to assess the model. There is an easy way to do using the method `score()` of the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82784dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = logreg.score(X, y)  # Call a general purpose evaluation function and obtain a (quality ) score of the model\n",
    "print('Logit model achieves a score of {:.3f} %'.format(perf*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7202a-455b-44a3-9d16-78c40a5ea1a3",
   "metadata": {},
   "source": [
    "## Exercise 2: Diagnosing predictions\n",
    "A score of above 87 percent sounds good. Actually, it is not, and your task is to find out why. Let's break it down into pieces.\n",
    "\n",
    "### A) What score?\n",
    "Find out what metric the function `score()` has provided. What specifically is this number of about 87 percent? You can check out the [sklearn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html) or any other source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03225801-d197-471b-a2be-c88aff8d126e",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a24e3-05d9-4439-8958-f7459cd0af4f",
   "metadata": {},
   "source": [
    "### B) Is it good or is it bad?\n",
    "Interpreting our score will be easier if we compare it to a baseline. But what baseline? We face a classification problem. There are two classes, good payers and bad payers, and we aim to tell these apart. Come up with a very basic strategy to solve the classification problem without using any model. Write a piece of code to calculate the performance of your super-basic strategy. \n",
    "\n",
    "<details> <summary>Hint: </summary> If you feel a bit lost, consider web searching for <i>dummy classifier</i> </details>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca09e4-5475-48f5-b938-10d8d6cd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to 2b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c128e4-0a74-47fc-b9d7-c54f894caec5",
   "metadata": {},
   "source": [
    "If you succeeded with the previous task, you will have found that a super-basic - stupid - classifier achieves an accuracy of 80 percent. This is not as high as what we saw from logit but it puts the first impression of logistic regression performing very well into perspective. \n",
    "\n",
    "Note that our approach to compute the score of the naive classifier assumes that the positive class with $Y=1$ is the minority class. While this is typically the case, we should acknowledge that our approach is simplistic. It would be better to first establish which of the two classes is the majority class and to then use the fraction of that class as the accuracy score of a dummy classifier. While not too difficult, we leave this extension for the interested to perform and move on with probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ac70-c675-4eae-951a-2210ee923dc8",
   "metadata": {},
   "source": [
    "### C) What about probabilities?\n",
    "Exactly, what about probabilities? The lecture introduced classification as a machine learning setup aimed at predicting class membership probabilities. So logistic regression should answer questions such as \"what is the estimated probability of the first credit applicant in our data set to repay?\" Guess what is your next task? Write code that provides you with the probabilistic predictions of the logistic regression classifier. More formally, your code should give you estimates of class membership probabilities $\\hat{p}(BAD==1|X_1)$ for all observations in the data set. \n",
    "\n",
    "To that end, begin experimenting with the method `predict()` that every learning algorithm in `sklearn` provides. Understand what it does and how it differs from what you need. Afterwards, search for the right method to obtain class membership probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a7cb-f1d2-4981-b0e0-7c73eb457a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to 2C\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
