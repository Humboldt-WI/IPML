{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d2a680-489a-4be4-b0bb-a1601079660b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/IPML/blob/master/tutorial_notebooks/t6_regression_solution.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0157e3-1b74-4592-8494-86ac3be129e0",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# A Supervised Learning Example: The linear Regression Model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wIgF2_GabxOZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1695536714957,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "wIgF2_GabxOZ"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23538915-b8b4-465b-9d6e-e6539f238144",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## The California Housing data set\n",
    "\n",
    "The \"California Housing\" data set is a widely used data set to demonstrate forecasting methods. As the name suggests, the data set concerns the valuation of real estate. It comprises socio-demographic information concerning the area of a property and a dependent (aka target) variable, which gives the median house value for California districts. This data was derived from the 1990 U.S. census. \n",
    "\n",
    "Being so popular, the data set is readily available in standard Python libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4cc9d-5bb8-47cc-948a-4ab6413a6189",
   "metadata": {
    "executionInfo": {
     "elapsed": 1738,
     "status": "ok",
     "timestamp": 1695536716691,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "1df4cc9d-5bb8-47cc-948a-4ab6413a6189"
   },
   "outputs": [],
   "source": [
    "# Downloading the data set\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing(as_frame=True)  # get the data as a Pandas dataframe\n",
    "\n",
    "print(california_housing.DESCR)  # the data even comes with a nice description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0489dd6-54fd-4ac2-ae35-6385427131dd",
   "metadata": {},
   "source": [
    "The `sklearn` library provides the data in a specific format. Feature values and the target variable are already separated. The be consistent with our standard notation, we extract the data and store it, as usual, in variables $X$ and $y$. Of course, this is a good opportunity to also take a quick look into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf24cd-e9b2-4264-8b99-7f7c1096f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = california_housing.data\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdbb36-d876-473c-9f8d-103e3d2ddad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = california_housing.target\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88f2f1-c54e-4a69-a159-96a64b3daec6",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada977f-144d-4163-87f6-b47238761c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute descriptive statistics that summarize the \n",
    "# target's distribution\n",
    "y.describe()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a3bc8-b53d-43bb-a5b7-0340ac3a4bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.describe() # do the same for all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a41e3b-0ea4-4697-a921-84b1651ec36d",
   "metadata": {},
   "source": [
    "### Explanatory data analysis\n",
    "To better understand the data, we consider the standard EDA operations introduced in the previous practical. They comprise the analysis of histograms and/or box-plots of the target and the features. \n",
    "\n",
    "Contrary to the previous demo in which our target variable was a binary class indicator, our target in the house value estimation setting is a real number. Therefore, we plots with grouping to, e.g., depict the distribution of a feature across different values of the target are inapplicable. Instead, we consider the (linear) correlation between features and the target to obtain some initial evidence as to which features might be important. \n",
    "\n",
    "While we only create the plots in the following, never forget that each plot and more generally every result deserves a careful analysis and discussion. Therefore, make sure to examine each plot and note down your key observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e420d55-5e3a-474f-b53d-f87c72afcfd7",
   "metadata": {},
   "source": [
    "#### Target Variable: Medium House Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WXjFWs3zF31A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1695536838762,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "WXjFWs3zF31A",
    "outputId": "c5473e54-8a6c-4908-f220-f0c7034d04e9"
   },
   "outputs": [],
   "source": [
    "# Histogram of target \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(y, bins=30, edgecolor='k')\n",
    "plt.xlabel(y.name)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Distribution of {y.name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618de746-2510-4f06-a083-865162740f15",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e9f0a-0c59-4ab2-89be-735839017bf2",
   "metadata": {},
   "source": [
    "The easiest way to produce a visualization of the distribution of all features is to call a `Matplotlib` plotting function via the data frame object. For example, to create a histogram of all (numerical) columns in the data frame, you can call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf29929-8f3c-4ef8-8ef1-4b87ceed8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(figsize=(10,8))  # Increase figure size size the call will produce many sub-plots\n",
    "plt.subplots_adjust(wspace=0.5,hspace=0.5)  # adjust the spacing of the sub plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6823a-1faf-47a3-ab95-f8b0600e332f",
   "metadata": {},
   "source": [
    "Being so convenient, it is tempting to use the same approach for other plots such as, for example, creating a boxplot of all features. However, this is less useful, as the below example shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713449e7-3f5c-45a0-9eb0-f1d18828527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of all features in a data frame\n",
    "X.boxplot(figsize=(8,4)) \n",
    "plt.subplots_adjust(wspace=0.5,hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb2a4a-0643-4803-a348-276ef824cd3b",
   "metadata": {},
   "source": [
    "So the problem lies in the different scale of the features. By default, `Pandas` uses a shared y-axis for all features. This renders the plot unreadable. We could remedy the problem that the feature values have different scale by first normalizing the feature values as discussed in the lecture on [Explanatory Data Analysis](https://moodle.hu-berlin.de/pluginfile.php/5910736/mod_resource/content/2/ipml_s3_eda.pdf). \n",
    "\n",
    "Alternatively, to create a boxplot for all (numerical) features, we can simply write a loop and plot each feature individually. This would nicely work when using `Matplotlib` but since we have to write a bit more code anyway, and obtain more control over our individual plots, we use `seaborn` in the below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xdXLDm-OF4Sn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1695536812007,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "xdXLDm-OF4Sn",
    "outputId": "745d4107-6eed-431a-c3fa-1ed56aea2b05"
   },
   "outputs": [],
   "source": [
    "#3x3 matrix of box-plots for all the features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))  # Create a 3x3 grid of subplots\n",
    "axes = axes.flatten() # Flatten the axes for easier iteration\n",
    "\n",
    "# Loop through each feature and create a box plot\n",
    "for i, feature in enumerate(X.columns):\n",
    "    sns.boxplot(x=X[feature], ax=axes[i], color='skyblue')\n",
    "    axes[i].set_title(f'Box Plot of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(X.columns), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d3ad5-13fa-45a2-8226-79c3aeee595e",
   "metadata": {},
   "source": [
    "#### Correlation analysis\n",
    "Having obtained an idea about the *univariate* distributions, we also take a look at the correlation structure in the data. For start, we consider the *Pearson* correlation coefficient, which we can compute for all pairs of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e09df1-2978-4e25-a0d9-4a06037729fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1695536838139,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "e_t4YznoF4AF",
    "outputId": "9e7d58d7-47c6-4418-db79-24b74d2f2afb"
   },
   "outputs": [],
   "source": [
    "# Correlation among the features\n",
    "corr_matrix = X.corr()  \n",
    "sns.heatmap(corr_matrix, cmap='vlag', annot=True)\n",
    "plt.title('Feature correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8d708-bc4b-4c66-89bf-415bdd68b3d0",
   "metadata": {},
   "source": [
    "A powerful way to obtain more insight into the dependency structure of features is the `pairplot()`, which integrates a visualization of individual feature's distribution and their association. The latter is depicted using scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f715548-491a-4b20-a72b-e2c7c6365f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96718c7-c405-40f3-833b-3f3398f4e5ff",
   "metadata": {},
   "source": [
    "Finally, it is also interesting - and in fact essential - to examine the correlation between features and the target variable. Since our target column is no longer part of the data frame, we can calculate (and visualize) this correlation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7cc71-9e52-4f7f-905f-e769468987c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between individual features and the target\n",
    "rho = X.corrwith(y)\n",
    "# Visualization by means of a bar plot \n",
    "plt.barh(X.columns.tolist(), rho)\n",
    "plt.title('Feature to target correlation')\n",
    "plt.xlabel('Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47300a-b43f-4482-a20a-034d18f92a0c",
   "metadata": {
    "id": "64fizQ6g6rQg"
   },
   "source": [
    "## Linear Regression\n",
    "Having completed our initial data screening and explorative analysis, we proceed by estimating a linear regression model to deepen our understanding of how the features and the target are related to another.\n",
    "\n",
    "To that end, we consider the library `statsmodels`, a popular and powerful library for statistical modeling, which includes ordinary least squares (OLS) estimator. We provide all codes for the model estimation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e353f01-f945-4734-bbf5-22ac905d10f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1695536839415,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "edVKGWXfgWtf"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "\n",
    "# OLS model estimation\n",
    "lr_main = sm.OLS(y,\n",
    "               sm.add_constant(X)  # include an intercept\n",
    "              ).fit() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9c41-e584-455f-8081-4f4d86a55631",
   "metadata": {
    "id": "64fizQ6g6rQg"
   },
   "source": [
    "The library provides a neat function, `.summary()` to obtain a concise overview of the results of regression analysis. It includes key information like R-squared, estimated coefficients, standard errors, and p-values. This summary is crucial for evaluating model adequacy and feature significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d635c-b74b-4dde-9c41-1ebe06c9c392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1695536839415,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "7GQ935p6WhxW",
    "outputId": "d0a8fb32-c2a8-41b6-8141-fc82fb6402ca"
   },
   "outputs": [],
   "source": [
    "print(lr_main.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbf141-f21e-4c9e-9c94-51b07f1f634e",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# Exercises\n",
    "Based on the above analysis, draw on your data science expertise to answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4373d5-09a1-430d-a6c2-41ea75601c3b",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## 1. What are the two most and the two least important features?\n",
    "Briefly note what statistics/results you have considered to make your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2714140-c525-4909-abca-c0b03412ad9d",
   "metadata": {},
   "source": [
    "**Your answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c03ef4-6116-4ae0-8118-0f3f1d0e9087",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## 2. Scatter Plots\n",
    "Create, for each of the selected features, a scatter plot of the selected feature against the target. Display these plots in a 2x2 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36307d9a-8f63-44b1-a145-b622a2c89d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445b153-5b13-4903-9c38-a97aa6f27a09",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## 3. Model Reestimation\n",
    "Remove the two least important features from the data and reestimate the model. Briefly discuss whether this step has improved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908535cb-275a-49a5-9e11-772144f60b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to reestimate the model using a new data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa95dc0-f8f9-4084-95cb-39cb60e4dbf5",
   "metadata": {},
   "source": [
    "**Did the model improve? Briefly discuss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c0969-9139-44a8-9e5e-b8b557f18f9b",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## EXTRA: Backward Elimination \n",
    "Relying on the following pseudo code, implement a *backward elimination* procedure, in which you repetitively discard the least important feature from the model.\n",
    "\n",
    "```\n",
    "    Use all features as the current set of features  \n",
    "    Do \n",
    "        Estimate a regression model using current set of features\n",
    "        Store model performance in terms of a suitable statistic\n",
    "        Identify the least important feature\n",
    "        Discard that feature from current set of features\n",
    "    Repeat untill all features but one have been deleted\n",
    "\n",
    "```\n",
    "Depict your results graphically by plotting the number of features in the regression model against model performance using the same statistic as in your backward elimination algorithm.\n",
    ">Hint: given you have to run the code inside the above *Do ... Repeat* block multiple times, consider implementing this part as a custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8108d5e-069b-4ee2-957b-2e4581f3503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward elimination code:\n",
    "# ---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9c155-83a8-4a5c-a341-7433b354734a",
   "metadata": {},
   "source": [
    "## 5. Switching the Library\n",
    "We have used `statsmodels` up to this point. However, the go-to library when it comes to machine learning is a different library called [scikit-learn](https://scikit-learn.org/stable/), typically abbreviated as sklearn.\n",
    "\n",
    "Import that library. Then, using once more the data set with all features included, create another linear regression model using the class `linear_model.LinearRegression()`. Compare the coefficients between this model and the one we estimated above using `statsmodels.api.OLS`. They should be pretty much identical. Please verify that they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5f53e-7be4-47ce-b9f0-719f3f17882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to estimate linear regression using sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b746e-5cfc-4389-820a-20e274ee0119",
   "metadata": {},
   "source": [
    "## 6. Prediction \n",
    "We next use our regression model for prediction. Feeding it with data on feature values, the estimated regression coefficients facilitate forecasting real-estate prices. More specifically, the lecture has introduced you to the process of evaluating prediction performance and its key ingredients, indicators of forecast accuracy and practices to organize available data so as to mimic a real-world application of a model. \n",
    "\n",
    "<img alt=\"Holdout method\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/holdout_method.png\" width=600 height=800/>\n",
    "\n",
    "As far as ingredient one is concerned, we will consider the mean squared error (MSE), defined as:<br> \n",
    "$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} \\left( Y_i - \\hat{Y}_i \\right)^2 $, <br>\n",
    "with:\n",
    "- $n$ = number of data points\n",
    "- $Y$ = true values of the target variable\n",
    "- $\\hat{Y}$ = forecasts of the regression model\n",
    "\n",
    "> Hint: if unsure how to implement the MSE yourself, you can be sure that ready-made functions are available to do it for you\n",
    "\n",
    "As the equation shows, given a trained (regression) model, we need a way to compute predictions $\\hat{Y}$. Your first task is to find a suitable function for this task. Use web-search, ask ChatGPT, or use some other way of your liking. Once you found a suitable function, write some code to try it out and understand how, in general, it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fdcb3-64a6-4dd9-a5b4-4d6b2fa10f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to compute model predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1f452-af19-429d-a4a5-d4ea7b5cbbea",
   "metadata": {},
   "source": [
    "Now that we know how, in principle, we can apply a trained model to (novel) data, we are ready to properly assess the model's accuracy. As shown by the above picture, this entails applying the model to some data, called the test in the above picture, then comparing model predictions to actual values of the target variable, and then aggregating the residuals using some error measure. We already agreed on using the MSE for the last step. Hence, unsurprisingly, your next task is implement the **holdout method**, which the picture shows. Specifically, write code to: \n",
    "1. Randomly split your data into a training set (70%) and a test set (30%)\n",
    "2. Train a regression model using the training set\n",
    "3. Compute forecasts for the test set\n",
    "4. Compute the test set MSE of the regression model\n",
    "\n",
    "> Hint: you have seen allmost everythihg that it takes to solve this task. The only missing bit is the partitioning of the data. You could write code to do it *manually* but given that this kind of data splitting is super common, there must be a ready-to-use function to do it and, as always, the real task is to find the right function to do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc3e57-bbe7-4cd9-be90-12c014622ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Randomly split your data into a training set (70%) and a test set (30%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570dc1b-6444-4d0e-91b8-b8f90ac8a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train a regression model using the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578ab45-8ecb-4948-81df-139820bee507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute forecasts for the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PygNvTZELmOG",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1695536841981,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "PygNvTZELmOG"
   },
   "outputs": [],
   "source": [
    "# 4. Compute the test set MSE of the regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c0400-e04a-4061-9c05-067e54d9c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.sum(e)/len(e)\n",
    "print(f'MSE of the regression models is {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fbac2-20b8-485d-bd0e-84756877eff0",
   "metadata": {},
   "source": [
    "Try to interpret the test set MSE of the model. What does the obtained value tell you?\n",
    "\n",
    "Hopefully, thinking about this question has reminded you of the lecture, in which we discussed how the MSE is difficult to interpret. Therefore, compute the root mean-square error $\\sqrt{MSE}$ and interpret this quantity. Do you agree that it is easier to interpret? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ae028-6755-4b07-94fb-08461354c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE of the regression model\n",
    "print('RMSE of the regression models is {}'.format(np.sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c76cc1-53a0-4399-92fe-99db5b2680d0",
   "metadata": {},
   "source": [
    "**Your interpretation of the RMSE:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef743b6-0b8d-4c6c-acb9-e6b0e6236f32",
   "metadata": {},
   "source": [
    "# 7. Cross-validation\n",
    "The MSE of the regression model gives us a solid estimate of how well the model predicts medium house values, that is our target. Before diving into more advanced means of forecast accuracy evaluation, let's take a step back and check whether the assessment of the model on a holdout data set was necessary in the first place. To that end, write code to compute the training set RMSE of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d345b-751e-42f6-8728-1b11f0650045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate training set RMSE of the regression model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Helper function\n",
    "def rmse(y_true, yhat):\n",
    "    \"\"\" Helpfer function to calculate root mean-square error \"\"\"\n",
    "    mse = mean_squared_error(y_true=y_true, y_pred=yhat)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "# Compute training set predictions\n",
    "y_hat_tr = sk_lr.predict(Xtr)\n",
    "\n",
    "rmse_tr = rmse(ytr, y_hat_tr)\n",
    "print('Training set RMSE of the regression models is {}'.format(rmse_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffeac1-86a3-427e-bfae-8c10bed8c5af",
   "metadata": {},
   "source": [
    "It is worth thinking about and discussing why the *training set* RMSE might not differ that much from the *test set* RMSE. This discussion, however, is out of the scope of this tutorial. Kindly accept that our result is a special case and that it would be very wrong to conclude that an evaluation of forecast accuracy on *fresh* data using the holdout method is dispensable. If you'd like to know more, we invite you to ask ChatGPT for its perspective on the matter. The very task of phrasing a suitable prompt would be an excellent exercise to help you to advance your ML skills. \n",
    "\n",
    "Provided we accept that we routinely assess forecast accuracy using out-of-sample data (that is any data but the training set), we should examine how the standard way to undertake this assessment. Entry k-fold cross-validation, which the lecture introduced as the process of randomly partitioning a data set into $k$ disjoint parts, training a model on the union of $k-1$ parts of the data, assessing the resulting model on the left out part, and repeating the process $k$ times while selecting a different part of the data as validation fold in each run: \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a17018-51b9-47ee-8122-d223fc567ed9",
   "metadata": {},
   "source": [
    "<img alt=\"Cross-validation\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/cross_validation1.png\" width=600 height=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ef2b7-9f5f-44bc-a523-3e398937c226",
   "metadata": {},
   "source": [
    "<br>\n",
    "Once the process of training and assessing $k$ models has been completed, we can integrate the out-of-sample predictions and compute the cross-validation error of the model on the resulting data.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879814c-2ece-4f1f-83c2-191d1e4b1930",
   "metadata": {},
   "source": [
    "<img alt=\"Cross-validation-based performance assessment\" src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/cross_validation2.png\" width=600 height=800/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf25bc-289f-46c4-8428-8cda11890d32",
   "metadata": {},
   "source": [
    "The `sklearn` library supports k-fold cross-validation in several ways. Generally speaking, these options differ in how much flexibility they leave you to adjust the code and how easy they are to use. In this notebook, we begin the perhaps easiest approach toward cross-validation. To that end, we use the function `cross_val_score()` which is available in the module `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b94a4-6938-45e1-b648-d5536c936fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function cross_val_score from the sklearn module sklearn.module_selection\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150cb52-c443-40fd-a723-0caf10eb05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the documentation of the function cross_val_score to understand how to use it and compute the cross-validation performance of your regression model using that function.\n",
    "?cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bea37-0171-4b89-bf84-7d9016bb1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv = cross_val_score(estimator=LinearRegression(), \n",
    "                X=X,  # Note how we must use the entire data set here, as the function does the partitioning internally and would otherwise cross-validate the training data\n",
    "                y=y,\n",
    "                cv=10,  # Ten-fold cross-validation\n",
    "                scoring='neg_root_mean_squared_error'  # Specify the accuracy indicator explicitly \n",
    "               )\n",
    "# The cross_val_score function assumes that we use accuracy measures; not error measures. Therefore, we have to multiply results by -1 to obtain estimates of the RMSE\n",
    "rmse_cv = -1* rmse_cv\n",
    "print('Cross-validation performance of the model in terms of RMSE:')\n",
    "print(rmse_cv)\n",
    "print('Average RMSE performance equals {:.2f}'.format(np.mean(rmse_cv)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
