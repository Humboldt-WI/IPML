{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9e8d57",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/IPML/blob/master/rpf_case_study_part-1.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679e745-23cc-4834-bd5f-b09122c84231",
   "metadata": {},
   "source": [
    "# Resale Price Forecasting Case Study - Part 1: Data Access and Exploration\n",
    "\n",
    "The lecture introduced you to resale price forecasting, a task to support decision-making in the leasing business. In this tutorial, we will explore a dataset representing resale price prediction. Throughout a series of tutorials, we will go through the different \n",
    "stages of a machine learning process, from initial data exploration to sophisticated predictive modeling and insightful post-hoc analysis. This notebook is only the start of a journey. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ec83c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13177d55-c2b2-4c9e-90b2-54b95480ac10",
   "metadata": {},
   "source": [
    "# Loading the Resale Price Prediction Dataset\n",
    "You can find the Resale Price Prediction dataset for this notebook on our Moodle page and in our GitHub repository. The Resale Price Prediction dataset focuses on laptops that have been leased and returned, aiming to predict their resale prices. The resale price is influenced by various factors, including the original retail price, depreciation, release year, screen size, hard drive size, RAM size, weight, lease duration, and battery capacity. Our final goal is to use the features to forecast resale prices. For start, however, we will explore the data to better understand its characteristics. Furthermore, data exploration facilitates learning about relevant Python libraries. For example, we will use the `pandas` library. Pandas is a widely used library for data analysis and manipulation in Python, providing powerful tools for handling structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b811f1b-651f-47eb-8c68-41c563f6eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Load pandas library\n",
    "\n",
    "# Dataset URL\n",
    "url = 'https://raw.githubusercontent.com/Humboldt-WI/IPML/main/data/resale_price_dataset.csv'\n",
    "\n",
    "# Load data from URL\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa51e8c-27b6-49ef-8d32-e76c7ac8d3b5",
   "metadata": {},
   "source": [
    "Let's first take a look at the data. To that end, we use the function `.head()`, which creates a preview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67ac57-9e57-4c7f-a179-4d2cb6164f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e8bbb-a3e2-4fc2-ab57-80ffdd796b31",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "The pandas library offers various functions to compute descriptive statistics, which help us summarize and understand the main characteristics of our dataset. Descriptive statistics provide insights into the distribution, central tendency, and variability of the data, allowing us to quickly grasp its overall structure. Furthermore, the pandas library offers functions to understand the data types and identify missing values in our dataset.\n",
    "\n",
    "The relevant functions we will use for our first examination of the data are `pd.DataFrame.info()` and `pd.DataFrame.describe()`.\n",
    "\n",
    "The `pd.DataFrame.info()` function reveals the high-level structure of our data table. Note that pandas uses the term *data frame* to refer to a table. The `pd.DataFrame.info()` function provides information on the number of entries (eg, rows), the data types of each column, and the number of missing values if any. Understanding these details is crucial for further analysis of the data. \n",
    "\n",
    "The `pd.DataFrame.describe()` function computes a suite of summary statistics for each column in our dataset. Examples include the average of a column or its standard deviation. \n",
    "\n",
    "But why are these functions essential? Both ```info()``` and ```describe()``` help us establish a foundational understanding of our dataset's distribution, scale, and tendencies. While ```info()``` gives us a structural overview, ```describe()``` takes us a step further into the statistical nature of each column. By noting aspects like the mean, standard deviation, and minimum/maxium values, we can swiftly detect outliers, identify patterns, and formulate hypotheses for further investigation.\n",
    "\n",
    "Together, these methods serve as our initial checkpoint, ensuring that we're not only aware of the dataset's composition but also acquainted with its statistical properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072e1e0-6281-4319-84d9-811155906fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset structure and info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0107097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ef453-9a75-4d60-b72f-b899d309c58f",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "In this subsection, we take a graphical approach to understand our Resale Price Prediction dataset. For this purpose we first load the two most prominent libraries for data visualization – `Matplotlib` and `Seaborn`.\n",
    "\n",
    "**Matplotlib**: A foundational plotting library, Matplotlib is the granddaddy of Python visualization tools. It offers immense flexibility and allows us to create a wide variety of charts and plots with fine-grained control over every aspect of the visuals. Whether it's histograms, scatter plots, or line charts, Matplotlib provides the functionalities to craft them all with detailed customizations.\n",
    "\n",
    "**Seaborn**: Built on top of Matplotlib, Seaborn simplifies many visual tasks, making sophisticated plots accessible and understandable. It comes with built-in themes and color palettes that enhance the aesthetics of our visualizations. Seaborn is particularly adept at handling statistical graphics, making it easier to visualize complex datasets with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fe3ad-bb31-49a4-8e6f-3568e69c4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91c858",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "Our first stop is the world of histograms — a type of plot that lets us see the frequency distribution of a single variable. By plotting histograms for all the features in our dataset, we can visually grasp the distribution of data points and detect any skewness or anomalies that might exist. This understanding is crucial as it directly influences how certain machine learning models might perform with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c00e8f-b400-408a-b1f0-57bf9c57bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for all numeric columns\n",
    "data.hist(figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d489ab7-4e54-4ed5-a272-b3af9e208ac5",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "\n",
    "Next, we consider the correlation between table columns. Recall that *correlation* is a measure for how much two numerical random variables (e.g., table columns) are linearly related. To compute the pairwise correlation between all table columns, we can use the `corr()` function, which Pandas provides. Afterwards, we can visualize all the pairwise correlations as a heatmap for easy inspection. To achieve this, we will use the `heatmap` function from the Seaborn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39d43d-f3e7-464e-8c4d-3f6057bfaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix and heatmap\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
